File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\basic_query.sql
================================================================================
SELECT DISTINCT 
     a.dog,
     b.cat,
     c.cow
     INTO #ANIMALS
     FROM table_a a
     INNER JOIN (
      SELECT setsk, animalsk
      FROM table_b b
      WHERE setsk = 1920
     ) b ON a.setsk = b.setsk
     LEFT JOIN table_c c ON a.setsk = c.setsk;

    SELECT * FROM #ANIMALS;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\component_responsibilities.csv
================================================================================
﻿File/Folder,Responsibilities,Dependencies
cli.py,"Entry point, argument parsing, workflow orchestration","All converters, utils"
converters/base.py,"Define converter interface, common functionality","utils.config, utils.logging"
converters/cte.py,Temp table → CTE conversion logic,"parsers, utils"
parsers/sql_parser.py,"SQL statement splitting, tokenization, syntax analysis",(pure Python)
utils/config.py,Load/merge configurations from files/ENV/CLI,(YAML/ENV handling)
utils/logging.py,Configure logging system,Python logging
tests/,Verify component behavior,"pytest, fixtures"


################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\conversions.log
================================================================================
2025-02-09 19:40:50,895 - SQLConverterApp - INFO - Processing file: C:\Users\User\python_code\sql_conversion\test.sql
2025-02-09 19:40:50,899 - SQLConverterApp - INFO - Saved converted SQL to: converted
2025-02-10 06:44:35,052 - root - ERROR - Fatal error: Invalid input path: input.sql
2025-02-10 06:45:57,908 - root - ERROR - Fatal error: Invalid input path: test.sql
2025-02-10 06:46:16,291 - SQLConverterApp - INFO - Processing file: C:\Users\User\python_code\sql_conversion\sql-query-converter\sql_converter\test.sql
2025-02-10 06:46:16,291 - SQLConverterApp - INFO - Saved converted SQL to: output.sql
2025-02-10 06:48:58,965 - SQLConverterApp - INFO - Processing file: C:\Users\User\python_code\sql_conversion\sql-query-converter\sql_converter\test.sql
2025-02-10 06:48:58,965 - SQLConverterApp - INFO - Saved converted SQL to: output.sql
2025-02-10 07:01:27,981 - root - ERROR - Fatal error: Invalid input path: input.sql
2025-02-10 07:01:48,025 - SQLConverterApp - INFO - Processing file: C:\Users\User\python_code\sql_conversion\test.sql
2025-02-10 07:01:48,025 - SQLConverterApp - INFO - Saved converted SQL to: output.sql
2025-02-10 07:02:04,142 - SQLConverterApp - INFO - Processing file: C:\Users\User\python_code\sql_conversion\basic_query.sql
2025-02-10 07:02:04,142 - SQLConverterApp - INFO - Saved converted SQL to: output.sql
2025-02-10 07:02:45,659 - SQLConverterApp - INFO - Processing file: C:\Users\User\python_code\sql_conversion\basic_query.txt
2025-02-10 07:02:45,674 - SQLConverterApp - INFO - Saved converted SQL to: output.sql
2025-02-10 07:03:08,301 - SQLConverterApp - INFO - Processing file: C:\Users\User\python_code\sql_conversion\sql_query_full.txt
2025-02-10 07:03:08,316 - SQLConverterApp - INFO - Saved converted SQL to: output.sql
2025-02-25 13:42:42,064 - SQLConverterApp - INFO - Processing file: C:\Users\User\python_code\sql_conversion\test.sql
2025-02-25 13:42:42,064 - SQLConverterApp - INFO - Saved converted SQL to: C:\Users\User\python_code\sql_conversion\test.sql


################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\converted
================================================================================
SELECT * INTO #temp FROM users;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\execution.txt
================================================================================
User → cli.py → ConfigManager → SQLConverterApp → CTEConverter → SQLParser
            ↳ Logging          ↳ File I/O         ↳ PivotConverter

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\extract_code.py
================================================================================
import os

def collect_all_files(root_dir, output_file):
    with open(output_file, 'w', encoding='utf-8') as out_f:
        for dirpath, _, filenames in os.walk(root_dir):
            if "__pycache__" in dirpath:
                continue  # Skip __pycache__ directories
            for filename in filenames:
                if filename == output_file:
                    continue
                file_path = os.path.join(dirpath, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                        content = file.read()
                        out_f.write(f"File: {file_path}\n")
                        out_f.write("=" * 80 + "\n")
                        out_f.write(content + "\n\n")
                        out_f.write("#" * 80 + "\n\n")
                except Exception as e:
                    print(f"Could not read {file_path}: {e}")

if __name__ == "__main__":
    root_directory = input("Enter the directory path to scan: ")
    output_filename = "collected_files.txt"
    collect_all_files(root_directory, output_filename)
    print(f"All files collected and saved in {output_filename}")

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\file-structure.txt
================================================================================
sql-query-converter/
├── sql_converter/
│   ├── __init__.py              # Package initialization
│   │
│   ├── cli.py                   # Command-line interface (entry point)
│   │                            # Depends on: converters, utils.config, utils.logging
│   │
│   ├── converters/              # Conversion logic implementations
│   │   ├── __init__.py          # Exports converter classes
│   │   ├── base.py              # BaseConverter (abstract class)
│   │   ├── cte.py               # CTEConverter (temp table → CTE)
│   │   └── pivot.py             # PivotConverter (future implementation)
│   │
│   ├── parsers/                 # SQL parsing components
│   │   ├── __init__.py          # Exports parser classes
│   │   ├── sql_parser.py        # SQLParser (statement splitting/tokenization)
│   │   └── tokenizer.py         # Advanced tokenization (if needed)
│   │
│   ├── utils/                   # Shared utilities
│   │   ├── __init__.py          # Utility exports
│   │   ├── config.py            # ConfigManager (configuration handling)
│   │   ├── logging.py           # Logging setup
│   │   ├── formatting.py        # SQL pretty-printing
│   │   └── helpers.py           # Generic helper functions
│   │
│   └── exceptions.py            # Custom exceptions
│
├── tests/                       # Unit tests
│   ├── __init__.py              # Test package
│   ├── test_cte_converter.py    # CTE converter tests
│   ├── test_sql_parser.py       # Parser tests
│   └── fixtures/                # Test SQL files
│       ├── input/               # Sample input SQL
│       └── expected/            # Expected output SQL
│
├── docs/                        # Documentation
│   ├── usage.md                 # User guide
│   └── api.md                   # Developer documentation
│
├── examples/                    # Usage examples
│   ├── basic_usage.py           # Simple API example
│   └── sample_queries/          # Example SQL files
│
├── scripts/                     # Maintenance scripts
│   ├── benchmark.py             # Performance testing
│   └── validate_config.py       # Config validation
│
├── config/                      # Default configurations
│   └── default.yml              # Base configuration
│
├── .gitignore                   # Version control ignore
├── pyproject.toml               # Build configuration
├── README.md                    # Project overview
└── requirements.txt             # Dependencies

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\output.sql
================================================================================
WITH distinct_deal_measure AS (
  SELECT DISTINCT 
      fdm.setsk, 
      fdm.dealsk, 
      buh.busunithlevel02name, 
      re.glreportingentitycode, 
      drea.glreportingentitycode AS affglreportingentitycode, 
      dmt.measuretypecode, 
      gla.glaccountcode, 
      dpt.productgroup
  FROM miris.dimdealset dds
  INNER JOIN (
      SELECT 
          setsk, dealsk, glbusinessunitsk, measuretypesk, glaccountsk, glreportingentitysk 
      FROM miris.factdealmeasure fdm 
      WHERE setsk = 15658
  ) fdm 
  ON dds.dealsk = fdm.dealsk AND fdm.setsk = 15658 AND dds.setsk = 15658
  LEFT JOIN dbo.gldimaccount gla 
  ON gla.glaccountsk = fdm.glaccountsk
  LEFT JOIN dbo.dimmeasuretype dmt 
  ON fdm.measuretypesk = dmt.measuretypesk
  LEFT JOIN dbo.dimproducttype dpt 
  ON dpt.producttypesk = dds.producttypesk
  LEFT JOIN dbo.gldimbusinessunithierarchy buh 
  ON fdm.glbusinessunitsk = buh.glbusinessunitsk 
  AND buh.busunithtrecode = 'bunit_stat' 
  AND buh.rowstatus = 'A'
  LEFT JOIN dbo.gldimreportingentity re 
  ON fdm.glreportingentitysk = re.glreportingentitysk
  LEFT JOIN dbo.gldimreportingentity drea 
  ON dds.glaffiliatereportingentitysk = drea.glreportingentitysk
  INNER JOIN (
      SELECT DISTINCT 
          reg.glreportingentitysk, 
          reg.glreportingentitycode, 
          cr.reportingconsumercode 
      FROM dbo.dimglreportingentitygroupmembership reg
      INNER JOIN dbo.dimglconsumerelevance cr
      ON reg.glreportingentitygroupcode = cr.glreportingentitygroupcode 
      AND cr.rowstatus = 'A'
      WHERE cr.reportingconsumercode LIKE '%EMEA%'
      AND reg.rowstatus = 'A'
  ) emea 
  ON emea.glreportingentitysk = dds.glreportingentitysk
  WHERE fdm.setsk = 15658
  AND NOT (
      (re.glreportingentitycode = drea.glreportingentitycode) 
      AND SUBSTRING(gla.glaccountcode, 5, 2) = '99'
  );
)


################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\requirements-test.txt
================================================================================
pytest>=7.0
pytest-mock>=3.0
freezegun>=1.0

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter.yml
================================================================================
# SQL Converter Configuration
# Enhanced for AST-based parsing and transformation

# Converters to apply (in order)
converters:
  - cte
  # - pivot (future support)

# Logging configuration
logging:
  level: DEBUG  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  file: conversions.log
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  console: true  # Output to console in addition to file

# New parser configuration
parser:
  # SQL dialect to use
  dialect: ansi  # ansi, tsql, mysql, postgresql, oracle, snowflake, redshift
  
  # Optimization level
  # 0: No optimization
  # 1: Basic optimizations (constant folding, dead code removal)
  # 2: Advanced optimizations (predicate pushdown, join reordering)
  optimization_level: 1
  
  # Whether to add schema information to the parse tree
  schema_aware: false
  
  # Treatment of syntax errors
  error_handling: strict  # strict, relaxed, recovery
  
  # Pretty printing options
  pretty_print:
    enabled: true
    indent_spaces: 2
    uppercase_keywords: true
    max_line_length: 100

# CTE converter configuration
cte_converter:
  # Indentation for CTEs
  indent_spaces: 2
  
  # Patterns to identify temp tables (regex simplified syntax)
  temp_table_patterns:
    - "#?temp_*"
    - "#?tmp_*"
    - "#.*"
  
  # CTE naming style
  cte_naming:
    strip_prefix: true    # Remove # and temp_ prefixes
    style: original       # original, snake_case, camelCase
  
  # Dependency handling
  dependency_handling:
    detect_cycles: true   # Detect circular dependencies
    auto_break_cycles: false  # Automatically break cycles (advanced)
  
  # AST-specific options
  ast:
    preserve_comments: true    # Try to preserve comments in conversion
    preserve_formatting: false  # Try to preserve original formatting

# Output configuration
output:
  default_output_dir: ./converted_sql
  overwrite: true
  backup: true       # Create backups of overwritten files
  format: true       # Format output SQL
  
  # Formatting options (if format is true)
  formatting:
    indent_spaces: 2
    uppercase_keywords: true
    max_line_length: 80
    comma_style: end    # end, start
    align_columns: true

# Advanced options
advanced:
  parallelism: 0  # 0 = auto, N = use N threads
  max_memory_mb: 0  # 0 = unlimited, N = limit to N MB
  timeout_seconds: 0  # 0 = unlimited, N = timeout after N seconds

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\test.sql
================================================================================
WITH temp AS (
  SELECT *
  FROM users;
)


################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\.pytest_cache\.gitignore
================================================================================
# Created by pytest automatically.
*


################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\.pytest_cache\CACHEDIR.TAG
================================================================================
Signature: 8a477f597d28d172789f06886806bc55
# This file is a cache directory tag created by pytest.
# For information about cache directory tags, see:
#	https://bford.info/cachedir/spec.html


################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\.pytest_cache\README.md
================================================================================
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\.pytest_cache\v\cache\lastfailed
================================================================================
{
  "sql_converter/tests/integration/test_integration.py::test_full_conversion": true,
  "sql_converter/tests/integration/test_integration.py::test_error_handling": true,
  "sql_converter/tests/unit/converters/test_cte.py::test_nested_temp_tables": true
}

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\.pytest_cache\v\cache\nodeids
================================================================================
[
  "sql_converter/tests/integration/test_cli.py::test_cli_file_conversion",
  "sql_converter/tests/integration/test_cli.py::test_cli_help",
  "sql_converter/tests/integration/test_integration.py::test_config_loading_from_multiple_sources",
  "sql_converter/tests/integration/test_integration.py::test_directory_structure_preservation",
  "sql_converter/tests/integration/test_integration.py::test_error_handling",
  "sql_converter/tests/integration/test_integration.py::test_full_conversion",
  "sql_converter/tests/unit/converters/test_cte.py::test_basic_cte_conversion",
  "sql_converter/tests/unit/converters/test_cte.py::test_multiple_temp_tables",
  "sql_converter/tests/unit/converters/test_cte.py::test_nested_temp_tables",
  "sql_converter/tests/unit/converters/test_cte.py::test_temp_table_pattern_matching",
  "sql_converter/tests/unit/parsers/test_sql_parser.py::test_comment_handling",
  "sql_converter/tests/unit/parsers/test_sql_parser.py::test_statement_splitting",
  "sql_converter/tests/unit/parsers/test_sql_parser.py::test_tsql_bracket_handling",
  "sql_converter/tests/unit/utils/test_config.py::test_config_loading",
  "sql_converter/tests/unit/utils/test_config.py::test_config_priority"
]

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\.pytest_cache\v\cache\stepwise
================================================================================
[]

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\cli.py
================================================================================
"""
SQL Converter CLI - Entry point for command-line SQL conversion tool.

This module provides the command-line interface and application orchestration
for converting SQL using AST-based parsing and transformation.
"""
import os
import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
import traceback

from sql_converter.converters import get_converter
from sql_converter.utils.config import ConfigManager
from sql_converter.utils.logging import setup_logging
from sql_converter.converters.base import BaseConverter
from sql_converter.parsers.sql_parser import SQLParser
from sql_converter.exceptions import (
    SQLConverterError, ConfigError, ValidationError, 
    SQLSyntaxError, FileError, ConverterError, ParserError
)


class SQLConverterApp:
    """
    Main application for SQL conversion using AST-based parsing and transformation.
    """
    
    def __init__(self, converters: Dict[str, BaseConverter], config: Dict[str, Any]):
        """
        Initialize the SQL Converter Application.
        
        Args:
            converters: Dictionary of converter name to converter instance
            config: Configuration dictionary
            
        Raises:
            ConfigError: When initialization fails due to config issues
        """
        if not converters:
            raise ConfigError("No converters provided")
            
        self.converters = converters
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # Initialize the parser with configured dialect
        dialect = config.get('parser', {}).get('dialect', 'ansi')
        self.parser = SQLParser(dialect=dialect)
        
        # Track processed files for reporting
        self.processed_files: Set[Path] = set()
        self.failed_files: Set[Tuple[Path, str]] = set()  # (path, error_message)
        
        # Configure optimization level
        self.optimization_level = config.get('parser', {}).get('optimization_level', 0)

    def process_file(self, input_path: Path, output_path: Path, conversions: List[str]) -> None:
        """
        Process a single SQL file using AST-based parsing and transformation.
        
        Args:
            input_path: Path to input SQL file
            output_path: Path to output SQL file
            conversions: List of converter names to apply
            
        Raises:
            FileError: When file operations fail
            ConverterError: When conversion fails
            ValidationError: When SQL validation fails
            ParserError: When SQL parsing fails
        """
        if not input_path.exists():
            raise FileError(f"Input file does not exist", filepath=str(input_path))
            
        if not input_path.is_file():
            raise FileError(f"Input path is not a file", filepath=str(input_path))
            
        # Check file access
        if not os.access(input_path, os.R_OK):
            raise FileError(f"No read permission for input file", filepath=str(input_path))
            
        # Ensure output directory exists and is writable
        output_dir = output_path.parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if output_dir.exists() and not os.access(output_dir, os.W_OK):
            raise FileError(f"No write permission for output directory", 
                           filepath=str(output_dir))
            
        try:
            self.logger.info(f"Processing file: {input_path}")
            
            # Read input file with proper error handling
            try:
                sql = input_path.read_text(encoding='utf-8')
            except UnicodeDecodeError:
                # Try with a different encoding
                try:
                    sql = input_path.read_text(encoding='latin-1')
                    self.logger.warning(f"File {input_path} was not UTF-8, read as Latin-1")
                except Exception as e:
                    raise FileError(f"Failed to read file: {str(e)}", 
                                   filepath=str(input_path)) from e
            except Exception as e:
                raise FileError(f"Failed to read file: {str(e)}", 
                               filepath=str(input_path)) from e

            # Parse SQL into AST expressions - NEW step with AST parser
            try:
                expressions = self.parser.parse(sql)
                self.logger.debug(f"Successfully parsed {len(expressions)} statements from {input_path}")
            except SQLSyntaxError as e:
                self.logger.error(f"SQL syntax error in {input_path}: {e}")
                raise
            except ParserError as e:
                self.logger.error(f"Parser error in {input_path}: {e}")
                raise

            # Apply conversions - Now working with AST expressions
            converted_expressions = expressions
            for conversion in conversions:
                if conversion not in self.converters:
                    raise ConverterError(f"Unknown converter: {conversion}")
                
                converter = self.converters[conversion]
                self.logger.debug(f"Applying converter '{conversion}' to {input_path}")
                
                # Apply the conversion with proper error handling
                try:
                    # Convert AST expressions - converter interface now accepts and returns AST
                    converted_expressions = converter.convert_ast(converted_expressions, self.parser)
                except Exception as e:
                    # Preserve error type if it's a known one, otherwise wrap
                    if isinstance(e, (SQLSyntaxError, ValidationError, ConverterError, ParserError)):
                        raise
                    raise ConverterError(
                        f"Error in {conversion} converter: {str(e)}",
                        source=input_path.name
                    ) from e

            # Convert AST expressions back to SQL
            try:
                converted_sql = "\n".join([self.parser.to_sql(expr) for expr in converted_expressions])
                self.logger.debug(f"Successfully converted AST back to SQL for {input_path}")
            except Exception as e:
                raise ConverterError(f"Error converting AST to SQL: {str(e)}")

            # Write output file with proper error handling
            try:
                output_path.write_text(converted_sql, encoding='utf-8')
                self.logger.info(f"Saved converted SQL to: {output_path}")
                self.processed_files.add(input_path)
            except Exception as e:
                raise FileError(f"Failed to write output file: {str(e)}", 
                               filepath=str(output_path)) from e

        except Exception as e:
            self.logger.error(f"Failed to process {input_path}: {str(e)}")
            self.failed_files.add((input_path, str(e)))
            raise

    def process_directory(self, input_dir: Path, output_dir: Path, conversions: List[str]) -> None:
        """
        Process all SQL files in a directory, preserving the directory structure.
        
        Args:
            input_dir: Directory containing SQL files to process
            output_dir: Directory to write converted SQL files to
            conversions: List of converter names to apply
            
        Raises:
            FileError: When directory operations fail
        """
        if not input_dir.exists():
            raise FileError(f"Input directory does not exist", filepath=str(input_dir))
            
        if not input_dir.is_dir():
            raise FileError(f"Input path is not a directory", filepath=str(input_dir))
            
        # Process sql files while preserving directory structure
        try:
            for input_path in input_dir.glob("**/*.sql"):
                if input_path.is_file():
                    # Calculate relative path to preserve directory structure
                    relative_path = input_path.relative_to(input_dir)
                    output_path = output_dir / relative_path
                    
                    try:
                        self.process_file(input_path, output_path, conversions)
                    except Exception as e:
                        self.logger.error(f"Error processing {input_path}: {e}")
                        # Continue processing other files
                        continue
                        
            # Check if we processed any files
            if not self.processed_files and not self.failed_files:
                self.logger.warning(f"No SQL files found in {input_dir}")
                
        except Exception as e:
            if isinstance(e, FileError):
                raise
            raise FileError(f"Error processing directory: {str(e)}", 
                           filepath=str(input_dir)) from e

    def get_summary(self) -> Dict[str, Any]:
        """
        Generate a summary of processing results.
        
        Returns:
            Dictionary with processing statistics
        """
        return {
            'processed_files': len(self.processed_files),
            'failed_files': len(self.failed_files),
            'success_rate': (
                len(self.processed_files) / 
                (len(self.processed_files) + len(self.failed_files))
                if (len(self.processed_files) + len(self.failed_files)) > 0 
                else 0
            ) * 100,
            'failures': [
                {'file': str(path), 'error': error} 
                for path, error in self.failed_files
            ]
        }


def main():
    """
    Main entry point for SQL converter CLI application.
    
    This function parses command-line arguments, initializes the application,
    and orchestrates the conversion process with comprehensive error handling.
    """
    # Initialize base logging before config
    logging.basicConfig(level=logging.WARNING)
    logger = logging.getLogger()
    
    try:
        # Initialize configuration
        config_manager = ConfigManager()
        
        try:
            config_manager.load_config()
        except ConfigError as e:
            logger.error(f"Configuration error: {e}")
            sys.exit(1)
            
        # Validate configuration
        validation_errors = config_manager.validate_config()
        if validation_errors:
            logger.warning("Configuration validation issues:")
            for error in validation_errors:
                logger.warning(f"  - {error}")

        # Setup logging with config
        try:
            setup_logging(
                level=config_manager.get('logging.level', 'INFO'),
                log_file=config_manager.get('logging.file')
            )
        except Exception as e:
            logger.error(f"Failed to configure logging: {e}")
            # Continue with basic logging

        # Parse command line arguments
        parser = argparse.ArgumentParser(
            description='SQL Query Conversion Tool',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter
        )
        
        parser.add_argument(
            '-i', '--input',
            type=Path,
            required=True,
            help='Input file or directory'
        )
        parser.add_argument(
            '-o', '--output',
            type=Path,
            required=True,
            help='Output file or directory'
        )
        
        # Get available converters before adding CLI arguments
        try:
            available_converters = [name for name in config_manager.get('converters', ['cte'])]
            
            parser.add_argument(
                '-c', '--convert',
                nargs='+',
                choices=available_converters,
                default=available_converters,
                help='Conversion operations to apply'
            )
        except Exception as e:
            logger.error(f"Failed to initialize converters list: {e}")
            available_converters = ['cte']  # Fallback
            
            parser.add_argument(
                '-c', '--convert',
                nargs='+',
                default=['cte'],
                help='Conversion operations to apply (failed to load converter list)'
            )

        # NEW: Add SQL dialect selection
        available_dialects = ['ansi', 'tsql', 'mysql', 'postgresql', 'oracle', 'snowflake', 'redshift']
        parser.add_argument(
            '-d', '--dialect',
            choices=available_dialects,
            default=config_manager.get('parser.dialect', 'ansi'),
            help='SQL dialect to use for parsing'
        )
        
        # NEW: Add optimization level
        parser.add_argument(
            '--optimize',
            type=int,
            choices=[0, 1, 2],
            default=config_manager.get('parser.optimization_level', 0),
            help='AST optimization level (0=none, 1=basic, 2=aggressive)'
        )

        # Add verbosity control
        parser.add_argument(
            '-v', '--verbose',
            action='store_true',
            help='Enable verbose output'
        )
        
        # Parse arguments
        try:
            args = parser.parse_args()
        except Exception as e:
            logger.error(f"Argument parsing error: {e}")
            parser.print_help()
            sys.exit(1)

        # Update verbosity
        if args.verbose:
            logger.setLevel(logging.DEBUG)
            for handler in logger.handlers:
                handler.setLevel(logging.DEBUG)

        # Update config with CLI arguments
        config_manager.update_from_cli(vars(args))
        
        # Add parser-specific config if not already present
        if 'parser' not in config_manager.config:
            config_manager.config['parser'] = {}
        config_manager.config['parser']['dialect'] = args.dialect
        config_manager.config['parser']['optimization_level'] = args.optimize

        # Initialize converters with config
        try:
            converters = {
                name: get_converter(name, config_manager.get(f"{name}_converter", {}))
                for name in config_manager.get('converters', ['cte'])
            }
        except Exception as e:
            logger.error(f"Failed to initialize converters: {e}")
            sys.exit(1)

        # Initialize application
        try:
            app = SQLConverterApp(converters, config_manager.config)
        except ConfigError as e:
            logger.error(f"Application initialization error: {e}")
            sys.exit(1)

        # Process input
        try:
            input_path = config_manager.get('input_path', args.input)
            output_path = config_manager.get('output_path', args.output)

            if input_path.is_file():
                if output_path.exists() and output_path.is_dir():
                    output_path = output_path / input_path.name
                app.process_file(input_path, output_path, args.convert)
                
            elif input_path.is_dir():
                app.process_directory(input_path, output_path, args.convert)
                
            else:
                raise FileError(f"Invalid input path: {input_path}")
                
            # Print summary
            summary = app.get_summary()
            logger.info(f"Processing complete: {summary['processed_files']} files processed, "
                      f"{summary['failed_files']} files failed "
                      f"({summary['success_rate']:.1f}% success rate)")
                      
            if summary['failed_files'] > 0:
                logger.warning("Failed files:")
                for failure in summary['failures'][:5]:  # Show the first 5 failures
                    logger.warning(f"  {failure['file']}: {failure['error']}")
                    
                if len(summary['failures']) > 5:
                    logger.warning(f"  ... and {len(summary['failures']) - 5} more failures")
                    
                # Exit with error code if there were failures
                sys.exit(1)

        except FileError as e:
            logger.error(f"File error: {e}")
            sys.exit(1)
        except ConverterError as e:
            logger.error(f"Converter error: {e}")
            sys.exit(1)
        except SQLSyntaxError as e:
            # Provide more specific error details with line numbers
            error_msg = f"SQL syntax error"
            if getattr(e, 'line', None):
                error_msg += f" at line {e.line}"
            if getattr(e, 'position', None):
                error_msg += f", position {e.position}"
            error_msg += f": {e.message}"
            
            logger.error(error_msg)
            sys.exit(1)
        except ParserError as e:
            logger.error(f"Parser error: {e}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            if args.verbose:
                logger.error(traceback.format_exc())
            sys.exit(1)

    except Exception as e:
        # Last resort error handling
        logger.error(f"Critical error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\exceptions.py
================================================================================
"""
Enhanced exception classes for SQL Converter with AST-based parsing support.

This module contains all exception types used throughout the SQL Converter application,
providing a consistent error handling approach with additional support for AST-based
parsing operations and more detailed error information.
"""
from typing import Optional, Any, Dict, List


class SQLConverterError(Exception):
    """Base exception for all SQL Converter errors."""
    
    def __init__(self, message: str, source: Optional[str] = None):
        self.source = source
        self.message = message
        super().__init__(f"{message} {f'[Source: {source}]' if source else ''}")


class ConfigError(SQLConverterError):
    """Raised when there's an issue with configuration."""
    pass


class ValidationError(SQLConverterError):
    """Raised when validation of inputs fails."""
    pass


class SQLSyntaxError(ValidationError):
    """
    Raised when SQL syntax is invalid, with enhanced AST information.
    
    Attributes:
        position: Character position where the error occurred
        line: Line number where the error occurred
        column: Column number where the error occurred
        ast_node: AST node where the error occurred (if available)
    """
    
    def __init__(
        self, 
        message: str, 
        source: Optional[str] = None, 
        position: Optional[int] = None, 
        line: Optional[int] = None,
        column: Optional[int] = None,
        ast_node: Optional[Any] = None
    ):
        self.position = position
        self.line = line
        self.column = column
        self.ast_node = ast_node
        
        # Build location information for the error message
        location_info = ""
        if line is not None:
            location_info += f" at line {line}"
            if column is not None:
                location_info += f", column {column}"
        elif position is not None:
            location_info += f" at position {position}"
        
        # Include AST node type if available
        node_info = ""
        if ast_node is not None:
            try:
                node_type = type(ast_node).__name__
                node_info = f" (node type: {node_type})"
            except:
                pass
        
        super().__init__(
            f"SQL syntax error{location_info}{node_info}: {message}", 
            source
        )


class ParserError(SQLConverterError):
    """
    Raised when there's an error during SQL parsing.
    
    Attributes:
        position: Character position where the error occurred
        line: Line number where the error occurred
        column: Column number where the error occurred
        token: Token information where the error occurred
    """
    
    def __init__(
        self, 
        message: str, 
        source: Optional[str] = None,
        position: Optional[int] = None,
        line: Optional[int] = None,
        column: Optional[int] = None,
        token: Optional[str] = None
    ):
        self.position = position
        self.line = line
        self.column = column
        self.token = token
        
        # Build location information
        location_info = ""
        if line is not None:
            location_info += f" at line {line}"
            if column is not None:
                location_info += f", column {column}"
        elif position is not None:
            location_info += f" at position {position}"
            
        # Include token information if available
        token_info = f" near '{token}'" if token else ""
        
        super().__init__(
            f"Parser error{location_info}{token_info}: {message}", 
            source
        )


class ASTError(SQLConverterError):
    """
    Raised when there's an error manipulating the SQL Abstract Syntax Tree.
    
    Attributes:
        node_type: Type of the AST node where the error occurred
        operation: Operation being performed when the error occurred
    """
    
    def __init__(
        self, 
        message: str, 
        source: Optional[str] = None,
        node_type: Optional[str] = None,
        operation: Optional[str] = None
    ):
        self.node_type = node_type
        self.operation = operation
        
        # Build context information
        context_info = ""
        if operation:
            context_info += f" during {operation}"
        if node_type:
            context_info += f" on {node_type} node"
            
        super().__init__(
            f"AST error{context_info}: {message}", 
            source
        )


class ConverterError(SQLConverterError):
    """
    Raised when there's an error during SQL conversion.
    
    Attributes:
        converter: Name of the converter that encountered the error
        stage: Conversion stage where the error occurred
        expressions: SQL expressions being converted (if applicable)
    """
    
    def __init__(
        self, 
        message: str, 
        source: Optional[str] = None,
        converter: Optional[str] = None,
        stage: Optional[str] = None,
        expressions: Optional[List[Any]] = None
    ):
        self.converter = converter
        self.stage = stage
        self.expressions = expressions
        
        # Build context information
        context_info = ""
        if converter:
            context_info += f" in {converter} converter"
        if stage:
            context_info += f" during {stage} stage"
            
        super().__init__(
            f"Converter error{context_info}: {message}", 
            source
        )


class FileError(SQLConverterError):
    """
    Raised when there's an issue with file operations.
    
    Attributes:
        filepath: Path to the file that caused the error
        operation: File operation that failed (read, write, etc.)
    """
    
    def __init__(
        self, 
        message: str, 
        filepath: Optional[str] = None,
        operation: Optional[str] = None
    ):
        self.filepath = filepath
        self.operation = operation
        
        # Build context information
        context_info = ""
        if operation:
            context_info += f" during {operation}"
        file_info = f" [File: {filepath}]" if filepath else ""
        
        super().__init__(f"{message}{context_info}{file_info}")


class TransformationError(SQLConverterError):
    """
    Raised when an AST transformation fails.
    
    Attributes:
        transformation: Name of the transformation that failed
        node_type: Type of the AST node being transformed
        details: Additional details about the transformation
    """
    
    def __init__(
        self, 
        message: str, 
        source: Optional[str] = None,
        transformation: Optional[str] = None,
        node_type: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ):
        self.transformation = transformation
        self.node_type = node_type
        self.details = details or {}
        
        # Build context information
        context_info = ""
        if transformation:
            context_info += f" during {transformation} transformation"
        if node_type:
            context_info += f" of {node_type} node"
            
        super().__init__(
            f"Transformation error{context_info}: {message}", 
            source
        )


class CTEError(TransformationError):
    """
    Raised when there's an error specific to CTE transformations.
    
    Attributes:
        temp_table: Name of the temporary table involved
        cte_name: Name of the CTE being created
    """
    
    def __init__(
        self, 
        message: str, 
        source: Optional[str] = None,
        temp_table: Optional[str] = None,
        cte_name: Optional[str] = None,
        **kwargs
    ):
        self.temp_table = temp_table
        self.cte_name = cte_name
        
        # Add additional details
        details = kwargs.pop('details', {})
        if temp_table:
            details['temp_table'] = temp_table
        if cte_name:
            details['cte_name'] = cte_name
            
        # Build table information
        table_info = ""
        if temp_table:
            table_info += f" for temp table '{temp_table}'"
            if cte_name:
                table_info += f" (CTE: '{cte_name}')"
                
        super().__init__(
            f"{message}{table_info}", 
            source,
            transformation="CTE conversion",
            details=details,
            **kwargs
        )


class PluginError(SQLConverterError):
    """Raised when there's an issue with a plugin or extension."""
    pass


class CircularDependencyError(ValidationError):
    """
    Raised when a circular dependency is detected in temp table references.
    
    Attributes:
        tables: List of tables involved in the circular dependency
    """
    
    def __init__(
        self, 
        message: str, 
        source: Optional[str] = None,
        tables: Optional[List[str]] = None
    ):
        self.tables = tables or []
        
        # Format the cycle for display
        cycle_display = ""
        if tables:
            cycle_display = " -> ".join(tables)
            if len(tables) > 1:
                cycle_display += f" -> {tables[0]}"  # Complete the cycle
            cycle_display = f" ({cycle_display})"
            
        super().__init__(
            f"Circular dependency detected{cycle_display}: {message}", 
            source
        )


class DialectError(ParserError):
    """
    Raised when there's an issue specific to SQL dialect handling.
    
    Attributes:
        dialect: SQL dialect being used
        feature: SQL feature that caused the error
    """
    
    def __init__(
        self, 
        message: str, 
        source: Optional[str] = None,
        dialect: Optional[str] = None,
        feature: Optional[str] = None,
        **kwargs
    ):
        self.dialect = dialect
        self.feature = feature
        
        # Build dialect information
        dialect_info = ""
        if dialect:
            dialect_info += f" in {dialect} dialect"
        if feature:
            dialect_info += f" ({feature} feature)"
            
        super().__init__(
            f"Dialect error{dialect_info}: {message}", 
            source,
            **kwargs
        )

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\__init__.py
================================================================================
__all__ = ['main']

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\converters\base.py
================================================================================
# File: sql-query-converter/sql_converter/converters/base.py
from abc import ABC, abstractmethod
from typing import Dict, Any
import logging

class BaseConverter(ABC):

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)

    @abstractmethod
    def convert(self, sql: str) -> str:
        """Convert SQL using this converter's logic"""
        pass

__all__ = ['BaseConverter']  # Add this at the bottom

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\converters\cte.py
================================================================================
"""
AST-based CTEConverter implementation for converting temporary tables to CTEs.

This module handles the transformation of SQL queries with temporary tables
into equivalent queries using Common Table Expressions (CTEs) by manipulating
the Abstract Syntax Tree rather than using regex pattern matching.
"""
import logging
import re
from typing import List, Dict, Any, Optional, Set, Tuple, Union

import sqlglot
from sqlglot import exp

from sql_converter.converters.base import BaseConverter
from sql_converter.parsers.sql_parser import SQLParser
from sql_converter.exceptions import ConverterError, ValidationError, ConfigError


class CTEConverter(BaseConverter):
    """
    Converts SQL queries with temporary tables to Common Table Expressions (CTEs)
    using AST-based analysis and transformation.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize CTEConverter with configuration.
        
        Args:
            config: Configuration dictionary for converter settings
        """
        super().__init__(config)
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # Configuration with defaults
        self.indent_spaces = self.config.get('indent_spaces', 4)
        temp_table_patterns = self.config.get('temp_table_patterns', ['#.*'])
        
        # Compile temporary table regex patterns
        try:
            self.temp_patterns = self._process_patterns(temp_table_patterns)
        except Exception as e:
            raise ConfigError(f"Failed to process temp table patterns: {str(e)}")
        
    def _process_patterns(self, patterns: List[str]) -> List:
        """
        Convert configuration patterns to compiled regex patterns.
        
        Args:
            patterns: List of pattern strings
            
        Returns:
            List of compiled regex pattern objects
            
        Raises:
            ConfigError: When pattern processing fails
        """
        if not patterns:
            raise ConfigError("No temp table patterns provided")
            
        compiled_patterns = []
        for i, pattern in enumerate(patterns):
            try:
                # Convert simplified pattern to regex
                processed = (
                    pattern.replace('?', '.?')
                           .replace('*', '.*')
                           .replace('#', r'\#')
                )
                compiled_patterns.append(re.compile(processed))
            except Exception as e:
                self.logger.warning(f"Invalid pattern '{pattern}' at index {i}: {str(e)}")
        
        if not compiled_patterns:
            self.logger.warning("No valid patterns found, using default pattern '#.*'")
            return [re.compile(r'\#.*')]
            
        return compiled_patterns

    def convert(self, sql: str) -> str:
        """
        Legacy method to maintain backward compatibility.
        Converts SQL with temp tables to use CTEs using the new AST-based approach.
        
        Args:
            sql: SQL query text to convert
            
        Returns:
            Converted SQL using CTEs
            
        Raises:
            ConverterError: For general conversion errors
            ValidationError: For validation errors
        """
        try:
            # Create a parser instance
            parser = SQLParser()
            
            # Parse the SQL into AST expressions
            expressions = parser.parse(sql)
            
            # Use the AST-based conversion method
            converted_expressions = self.convert_ast(expressions, parser)
            
            # Convert back to SQL text
            converted_sql = "\n".join([parser.to_sql(expr) for expr in converted_expressions])
            
            return converted_sql
            
        except Exception as e:
            error_msg = f"Failed to convert SQL: {str(e)}"
            self.logger.error(error_msg)
            raise ConverterError(error_msg) from e

    def convert_ast(self, expressions: List[exp.Expression], parser: SQLParser) -> List[exp.Expression]:
        """
        Convert SQL expressions with temp tables to use CTEs using AST manipulation.
        
        Args:
            expressions: List of AST expressions to convert
            parser: SQLParser instance for AST operations
            
        Returns:
            List of converted AST expressions
            
        Raises:
            ConverterError: For general conversion errors
            ValidationError: For validation errors
        """
        try:
            # Extract regex pattern strings for the parser
            pattern_strings = [p.pattern for p in self.temp_patterns]
            
            # Find temporary tables in the expressions
            temp_tables = self._identify_temp_tables(expressions, parser, pattern_strings)
            
            if not temp_tables:
                # No temp tables found, return the original expressions
                return expressions
                
            # Build dependency graph between temp tables
            dependency_graph = self._build_dependency_graph(temp_tables)
            
            # Order temp tables based on dependencies
            ordered_temp_tables = self._topological_sort(dependency_graph, temp_tables)
            
            # Separate main query from temp table definitions
            main_expressions, definition_expressions = self._separate_expressions(
                expressions, temp_tables
            )
            
            if not main_expressions:
                # If all expressions are temp table definitions, use the last one
                # as the main query (with temp references replaced)
                main_expressions = [self._deep_copy_expression(definition_expressions[-1])]
            
            # Create CTEs and apply transformations
            result = self._create_ctes_and_transform(
                main_expressions, ordered_temp_tables, parser
            )
            
            return result
            
        except Exception as e:
            if isinstance(e, (ValidationError, ConverterError)):
                raise
            error_msg = f"Failed to convert AST: {str(e)}"
            self.logger.error(error_msg)
            raise ConverterError(error_msg) from e

    def _identify_temp_tables(
        self, 
        expressions: List[exp.Expression],
        parser: SQLParser,
        pattern_strings: List[str]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Identify temporary tables and their definitions in AST expressions.
        
        Args:
            expressions: List of AST expressions to analyze
            parser: SQLParser instance for AST operations
            pattern_strings: List of regex pattern strings for temp tables
            
        Returns:
            Dictionary mapping temp table names to their definition info
        """
        # Use the parser to find temp tables
        temp_tables_info = parser.find_temp_tables("\n".join([parser.to_sql(expr) for expr in expressions]), pattern_strings)
        
        # Convert to our internal format
        temp_tables = {}
        for temp_info in temp_tables_info:
            name = temp_info['name']
            temp_tables[name] = {
                'name': name,
                'cte_name': self._get_cte_name(name),
                'type': temp_info['type'],
                'definition': temp_info['definition'],
                'defined_expr': temp_info['defined_expr'],
                'dependencies': temp_info['dependencies']
            }
            
        return temp_tables

    def _get_cte_name(self, temp_name: str) -> str:
        """
        Generate a CTE name from a temp table name.
        
        Args:
            temp_name: Original temp table name
            
        Returns:
            Cleaned name suitable for a CTE
        """
        return temp_name.lstrip('#').replace('.', '_')

    def _build_dependency_graph(self, temp_tables: Dict[str, Dict[str, Any]]) -> Dict[str, List[str]]:
        """
        Build a dependency graph between temp tables.
        
        Args:
            temp_tables: Dictionary of temp tables and their info
            
        Returns:
            Dictionary mapping temp tables to their dependencies
        """
        # Initialize the graph with empty dependency lists
        graph = {name: [] for name in temp_tables}
        
        # Add dependencies from the temp_tables info
        for name, info in temp_tables.items():
            for dep in info['dependencies']:
                if dep in temp_tables and dep != name:  # Avoid self-references
                    if dep not in graph[name]:
                        graph[name].append(dep)
        
        return graph

    def _topological_sort(
        self, 
        graph: Dict[str, List[str]], 
        temp_tables: Dict[str, Dict[str, Any]]
    ) -> List[str]:
        """
        Sort temp tables in dependency order using topological sort.
        
        Args:
            graph: Dependency graph of temp tables
            temp_tables: Dictionary of temp tables and their info
            
        Returns:
            List of temp table names in dependency order
            
        Raises:
            ValidationError: If a circular dependency is detected
        """
        # Track visited nodes for cycle detection
        permanent_mark = set()
        temporary_mark = set()
        result = []
        
        def visit(node):
            if node in permanent_mark:
                return
            if node in temporary_mark:
                raise ValidationError(f"Circular dependency detected involving {node}")
                
            temporary_mark.add(node)
            
            # Visit dependencies first
            for dependency in graph.get(node, []):
                visit(dependency)
                
            temporary_mark.remove(node)
            permanent_mark.add(node)
            result.append(node)
            
        # Visit all nodes
        for node in graph:
            if node not in permanent_mark:
                visit(node)
                
        # Return in reverse order (dependencies first)
        return list(reversed(result))

    def _separate_expressions(
        self, 
        expressions: List[exp.Expression],
        temp_tables: Dict[str, Dict[str, Any]]
    ) -> Tuple[List[exp.Expression], List[exp.Expression]]:
        """
        Separate main query expressions from temp table definition expressions.
        
        Args:
            expressions: List of all AST expressions
            temp_tables: Dictionary of temp tables and their info
            
        Returns:
            Tuple of (main_expressions, definition_expressions)
        """
        # Find expression objects that define temp tables
        definition_expr_set = set()
        for info in temp_tables.values():
            definition_expr_set.add(id(info['defined_expr']))
        
        # Separate main and definition expressions
        main_expressions = []
        definition_expressions = []
        
        for expr in expressions:
            if id(expr) in definition_expr_set:
                definition_expressions.append(expr)
            else:
                main_expressions.append(expr)
        
        return main_expressions, definition_expressions

    def _deep_copy_expression(self, expr: exp.Expression) -> exp.Expression:
        """
        Create a deep copy of an AST expression.
        
        Args:
            expr: AST expression to copy
            
        Returns:
            Deep copy of the expression
        """
        return expr.copy()

    def _create_ctes_and_transform(
        self,
        main_expressions: List[exp.Expression],
        ordered_temp_tables: List[str],
        parser: SQLParser
    ) -> List[exp.Expression]:
        """
        Create CTEs and transform main expressions to use them.
        
        Args:
            main_expressions: List of main query expressions
            ordered_temp_tables: List of temp table names in dependency order
            parser: SQLParser instance for AST operations
            
        Returns:
            List of transformed expressions using CTEs
        """
        if not ordered_temp_tables:
            return main_expressions
        
        # Create a map of original table names to CTE names
        replacements = {
            name: self._get_cte_name(name)
            for name in ordered_temp_tables
        }
        
        # Transform each main expression
        result = []
        for expr in main_expressions:
            # Replace table references in the main expression
            transformed_expr = parser.replace_references(expr, replacements)
            
            # If it's already a WITH expression, we need to add our CTEs to it
            if isinstance(transformed_expr, exp.With):
                # Extract the existing WITH expression's query
                with_query = transformed_expr.expression
                
                # Add our CTEs to the existing CTEs
                for name in ordered_temp_tables:
                    cte_name = replacements[name]
                    cte_def = self._temp_tables[name]['definition']
                    
                    # Replace references in the CTE definition
                    cte_def = parser.replace_references(cte_def, replacements)
                    
                    # Add to existing WITH expressions
                    transformed_expr.expressions.append(
                        exp.CTE(
                            this=exp.to_identifier(cte_name),
                            expression=cte_def
                        )
                    )
                
                result.append(transformed_expr)
            else:
                # Create a new WITH expression with our CTEs
                ctes = []
                for name in ordered_temp_tables:
                    cte_name = replacements[name]
                    cte_def = self._temp_tables[name]['definition']
                    
                    # Replace references in the CTE definition
                    cte_def = parser.replace_references(cte_def, replacements)
                    
                    ctes.append(
                        exp.CTE(
                            this=exp.to_identifier(cte_name),
                            expression=cte_def
                        )
                    )
                
                # Create the WITH expression with the transformed main query
                with_expr = exp.With(
                    expressions=ctes,
                    expression=transformed_expr
                )
                
                result.append(with_expr)
        
        return result

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\converters\__init__.py
================================================================================
# sql_converter/converters/__init__.py
from .base import BaseConverter
# Register default converters
from .cte import CTEConverter
__all__ = ['CTEConverter']
_converters = {}

def register_converter(name: str, converter_class: type):
    if not issubclass(converter_class, BaseConverter):
        raise TypeError("Converters must inherit from BaseConverter")
    _converters[name] = converter_class

def get_converter(name: str, config: dict = None) -> BaseConverter:
    if name not in _converters:
        raise ValueError(f"Unknown converter: {name}")
    return _converters[name](config=config)


register_converter('cte', CTEConverter)
# Future converters would be registered here
# from .pivot import PivotConverter
# register_converter('pivot', PivotConverter)

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\parsers\sql_parser.py
================================================================================
"""
Advanced SQL Parser implementation using sqlglot for proper AST parsing.

This module replaces the regex-based parser with a robust SQL parser that
creates a proper Abstract Syntax Tree (AST) representation of SQL code.
"""
import logging
from typing import List, Dict, Optional, Generator, Tuple, Union, Any
from pathlib import Path

import sqlglot
from sqlglot import parse, ParseError, TokenError, exp
from sqlglot.optimizer import optimize

from sql_converter.exceptions import SQLSyntaxError, ParserError


class SQLParser:
    """
    Parser for SQL statements using sqlglot for proper AST parsing.
    This class maintains the same interface as the original parser
    but adds AST-based capabilities.
    """
    
    # Map our dialect names to sqlglot dialects
    DIALECT_MAP = {
        'ansi': 'ansi',
        'tsql': 'tsql',
        'mysql': 'mysql',
        'postgresql': 'postgres',
        'oracle': 'oracle',
        'bigquery': 'bigquery',
        'snowflake': 'snowflake',
        'redshift': 'redshift',
        'spark': 'spark',
    }
    
    def __init__(self, dialect: str = 'ansi'):
        """
        Initialize the SQL parser with the specified dialect.
        
        Args:
            dialect: SQL dialect to use ('ansi', 'tsql', 'mysql', etc.)
        """
        self.dialect_name = dialect.lower()
        self.logger = logging.getLogger(__name__)
        
        # Map to sqlglot dialect
        self.dialect = self.DIALECT_MAP.get(self.dialect_name, 'ansi')
        self.logger.debug(f"Initialized parser with dialect: {self.dialect}")

    def parse(self, sql: str) -> List[exp.Expression]:
        """
        Parse SQL into AST expressions.
        
        Args:
            sql: SQL code to parse
            
        Returns:
            List of sqlglot Expression objects representing the parsed SQL
            
        Raises:
            SQLSyntaxError: When SQL contains syntax errors
            ParserError: When the parser encounters an error
        """
        try:
            # Parse the SQL into a list of expression trees
            expressions = parse(sql, dialect=self.dialect, error_level='raise')
            return expressions
        except ParseError as e:
            # Extract position information if available
            position = getattr(e, 'position', None)
            line = None
            if position:
                # Calculate line number from position
                line = sql[:position].count('\n') + 1
            
            # Raise our custom exception with detailed info
            raise SQLSyntaxError(
                str(e),
                source=sql[:100] + '...' if len(sql) > 100 else sql,
                position=position,
                line=line
            ) from e
        except TokenError as e:
            # Handle tokenization errors
            raise SQLSyntaxError(
                f"SQL tokenization error: {str(e)}",
                source=sql[:100] + '...' if len(sql) > 100 else sql
            ) from e
        except Exception as e:
            # Handle any other unexpected errors
            raise ParserError(
                f"Error parsing SQL: {str(e)}",
                source=sql[:100] + '...' if len(sql) > 100 else sql
            ) from e

    def validate_sql(self, sql: str) -> None:
        """
        Validates SQL syntax and raises specific SQLSyntaxError exceptions.
        
        Args:
            sql: The SQL statement to validate
            
        Raises:
            SQLSyntaxError: When SQL contains syntax errors
        """
        # Check for empty SQL
        if not sql or not sql.strip():
            raise SQLSyntaxError("Empty SQL statement", position=0, line=1)
        
        # Use the parser to validate syntax - this will raise appropriate exceptions
        self.parse(sql)
        
        # If we get here, the SQL is syntactically valid according to the parser

    def split_statements(self, sql: str, skip_validation: bool = False) -> List[str]:
        """
        Split SQL into individual statements using the parser.
        
        Args:
            sql: SQL code potentially containing multiple statements
            skip_validation: If True, skip initial SQL validation
            
        Returns:
            List of individual SQL statements
            
        Raises:
            SQLSyntaxError: When SQL contains syntax errors
            ParserError: When the parser encounters an error
        """
        # Don't validate the whole SQL if skip_validation is True
        # The parser will validate each statement separately
        if not skip_validation:
            try:
                self.validate_sql(sql)
            except SQLSyntaxError as e:
                self.logger.error(f"SQL validation error: {e}")
                raise

        try:
            # Parse SQL into AST expressions
            expressions = self.parse(sql)
            
            # Convert expressions back to SQL strings
            statements = [expr.sql(dialect=self.dialect) for expr in expressions]
            
            # Return non-empty statements
            return [stmt for stmt in statements if stmt.strip()]
            
        except Exception as e:
            # If parsing fails but we need to split anyway (skip_validation=True),
            # fall back to semicolon splitting as a best effort
            if skip_validation:
                self.logger.warning(f"AST parsing failed, falling back to semicolon splitting: {e}")
                return self._fallback_split_statements(sql)
            raise

    def _fallback_split_statements(self, sql: str) -> List[str]:
        """
        Fallback method to split SQL statements by semicolons.
        Used when AST parsing fails but we still need a best-effort split.
        
        Args:
            sql: SQL code to split
            
        Returns:
            List of SQL statements (best effort)
        """
        # Simple semicolon splitting - this won't handle quoted semicolons correctly
        raw_statements = sql.split(';')
        
        # Filter and clean statements
        statements = [stmt.strip() for stmt in raw_statements]
        return [stmt for stmt in statements if stmt]

    def tokenize(self, sql: str) -> Generator[Tuple[str, str], None, None]:
        """
        Tokenize SQL into meaningful components using the parser's tokenizer.
        
        Args:
            sql: SQL statement to tokenize
            
        Returns:
            Generator yielding (token_type, token_value) tuples
            
        Raises:
            ParserError: When tokenization fails
        """
        try:
            # Use sqlglot's tokenizer
            tokens = sqlglot.tokenize(sql, dialect=self.dialect)
            
            for token in tokens:
                # Map sqlglot token types to our expected types
                token_type = self._map_token_type(token.token_type)
                token_value = token.text
                
                # Skip whitespace tokens
                if token_type == 'WHITESPACE':
                    continue
                    
                yield (token_type, token_value)
                
        except Exception as e:
            # Convert any unexpected errors to ParserError
            raise ParserError(
                f"Error during SQL tokenization: {str(e)}",
                source=sql[:100] + '...' if len(sql) > 100 else sql
            ) from e

    def _map_token_type(self, sqlglot_token_type: str) -> str:
        """
        Map sqlglot token types to our expected token types.
        
        Args:
            sqlglot_token_type: Token type from sqlglot
            
        Returns:
            Mapped token type string
        """
        # Map from sqlglot token types to our types
        type_map = {
            'STRING': 'STRING',
            'NUMBER': 'NUMBER',
            'IDENTIFIER': 'IDENTIFIER',
            'KEYWORD': 'KEYWORD',
            'OPERATOR': 'OPERATOR',
            'L_PAREN': 'PAREN',
            'R_PAREN': 'PAREN',
            'SEMICOLON': 'SEMICOLON',
            'WHITESPACE': 'WHITESPACE',
            'COMMENT': 'COMMENT',
        }
        
        # Return mapped type or the original if not in map
        return type_map.get(sqlglot_token_type, sqlglot_token_type)

    def parse_identifiers(self, sql: str) -> List[str]:
        """
        Extract all identifiers from SQL query using the AST.
        
        Args:
            sql: SQL statement to extract identifiers from
            
        Returns:
            List of SQL identifiers found
            
        Raises:
            ParserError: When identifier extraction fails
        """
        try:
            # Parse the SQL into an AST
            expressions = self.parse(sql)
            
            # Extract identifiers from the AST
            identifiers = []
            for expr in expressions:
                # Use sqlglot's built-in traversal to find all identifiers
                for node in expr.find_all(exp.Identifier):
                    # Get the identifier name, handling qualified names
                    identifier = node.name
                    identifiers.append(identifier)
                    
            return identifiers
            
        except Exception as e:
            if isinstance(e, (SQLSyntaxError, ParserError)):
                raise
            # Convert other errors to ParserError
            raise ParserError(f"Error extracting identifiers: {str(e)}")

    def find_table_references(self, sql: str) -> List[Dict[str, Any]]:
        """
        Find all table references in the SQL using the AST.
        
        Args:
            sql: SQL statement to analyze
            
        Returns:
            List of dictionaries with table reference information
            
        Raises:
            ParserError: When table extraction fails
        """
        try:
            # Parse the SQL into an AST
            expressions = self.parse(sql)
            
            # Initialize results
            table_refs = []
            
            # Extract table references from all expressions
            for expr in expressions:
                # Find all table references (FROM, JOIN, etc.)
                for table in expr.find_all(exp.Table):
                    # Get reference information
                    ref_info = {
                        'table': table.name,
                        'alias': table.alias_or_name,
                        'schema': table.db,
                        'catalog': table.catalog,
                        'is_cte': isinstance(table.parent, exp.CTE),
                        'context': self._get_reference_context(table)
                    }
                    table_refs.append(ref_info)
                    
            return table_refs
            
        except Exception as e:
            if isinstance(e, (SQLSyntaxError, ParserError)):
                raise
            # Convert other errors to ParserError
            raise ParserError(f"Error finding table references: {str(e)}")

    def _get_reference_context(self, node: exp.Expression) -> str:
        """
        Determine the context in which a table is referenced.
        
        Args:
            node: AST node to examine
            
        Returns:
            Context string (FROM, JOIN, etc.)
        """
        # Walk up the tree to find the context
        parent = node.parent
        
        while parent:
            if isinstance(parent, exp.From):
                return 'FROM'
            elif isinstance(parent, exp.Join):
                return f'{parent.kind} JOIN'
            elif isinstance(parent, exp.Into):
                return 'INTO'
            elif isinstance(parent, exp.With):
                return 'WITH'
                
            parent = parent.parent
            
        return 'UNKNOWN'

    def find_temp_tables(self, sql: str, patterns: List[str]) -> List[Dict[str, Any]]:
        """
        Find temporary table definitions and references in the SQL.
        
        Args:
            sql: SQL statement to analyze
            patterns: List of patterns to identify temp tables
            
        Returns:
            List of dictionaries with temp table information
            
        Raises:
            ParserError: When extraction fails
        """
        try:
            # First compile patterns for temp table identification
            import re
            temp_patterns = [re.compile(pattern) for pattern in patterns]
            
            # Parse the SQL into an AST
            expressions = self.parse(sql)
            
            # Find all table definitions and references
            temp_tables = []
            
            for expr in expressions:
                # Look for SELECT INTO statements
                for select in expr.find_all(exp.Select):
                    if hasattr(select, 'into') and select.into:
                        table_name = select.into.name
                        
                        # Check if this is a temp table
                        if any(pattern.search(table_name) for pattern in temp_patterns):
                            # Create a definition record
                            temp_info = {
                                'name': table_name,
                                'type': 'SELECT_INTO',
                                'definition': select,
                                'defined_expr': expr,
                                'dependencies': self._find_dependencies(select, temp_patterns)
                            }
                            temp_tables.append(temp_info)
                
                # Look for CREATE TEMP TABLE statements
                for create in expr.find_all(exp.Create):
                    if hasattr(create, 'this') and create.this:
                        table_name = create.this.name
                        
                        # Check if this is a temp table
                        if any(pattern.search(table_name) for pattern in temp_patterns):
                            # Get definition type
                            if hasattr(create, 'expression') and create.expression:
                                definition_type = 'CREATE_TEMP_AS'
                            else:
                                definition_type = 'CREATE_TEMP'
                                
                            # Create a definition record
                            temp_info = {
                                'name': table_name,
                                'type': definition_type,
                                'definition': create,
                                'defined_expr': expr,
                                'dependencies': self._find_dependencies(create, temp_patterns)
                            }
                            temp_tables.append(temp_info)
            
            return temp_tables
            
        except Exception as e:
            if isinstance(e, (SQLSyntaxError, ParserError)):
                raise
            # Convert other errors to ParserError
            raise ParserError(f"Error finding temp tables: {str(e)}")

    def _find_dependencies(self, node: exp.Expression, temp_patterns: List) -> List[str]:
        """
        Find dependencies on other temp tables in a definition.
        
        Args:
            node: AST node to examine
            temp_patterns: List of compiled patterns for temp tables
            
        Returns:
            List of temp table names this definition depends on
        """
        dependencies = []
        
        # Find all table references in this node
        for table in node.find_all(exp.Table):
            table_name = table.name
            
            # Check if this is a reference to a temp table
            if any(pattern.search(table_name) for pattern in temp_patterns):
                dependencies.append(table_name)
                
        return dependencies

    def replace_references(self, expr: exp.Expression, replacements: Dict[str, str]) -> exp.Expression:
        """
        Replace table references in an AST expression.
        
        Args:
            expr: AST expression to modify
            replacements: Dictionary mapping original names to replacements
            
        Returns:
            Modified AST expression
        """
        # Make a copy of the expression to avoid modifying the original
        new_expr = expr.copy()
        
        # Replace all table references
        for table in new_expr.find_all(exp.Table):
            table_name = table.name
            if table_name in replacements:
                # Replace the table name with the new name
                table.set('this', exp.to_identifier(replacements[table_name]))
                
        return new_expr

    def to_sql(self, expr: exp.Expression) -> str:
        """
        Convert an AST expression back to SQL text.
        
        Args:
            expr: AST expression to convert
            
        Returns:
            SQL string
        """
        return expr.sql(dialect=self.dialect)

    def generate_cte(self, name: str, definition: exp.Expression) -> exp.With:
        """
        Generate a CTE expression from a subquery definition.
        
        Args:
            name: Name for the CTE
            definition: AST expression defining the CTE
            
        Returns:
            With expression representing the CTE
        """
        # Create a CTE node with the given name and definition
        if isinstance(definition, exp.Select):
            # For SELECT statements, use directly
            cte = exp.With(
                expressions=[
                    exp.CTE(
                        this=exp.to_identifier(name),
                        expression=definition
                    )
                ]
            )
        else:
            # For other statements, extract the SELECT part if possible
            select_part = definition.find(exp.Select)
            if select_part:
                cte = exp.With(
                    expressions=[
                        exp.CTE(
                            this=exp.to_identifier(name),
                            expression=select_part
                        )
                    ]
                )
            else:
                # Fallback - convert to SQL and parse as a subquery
                sql = f"SELECT * FROM ({self.to_sql(definition)}) AS subquery"
                parsed = self.parse(sql)[0]
                
                cte = exp.With(
                    expressions=[
                        exp.CTE(
                            this=exp.to_identifier(name),
                            expression=parsed
                        )
                    ]
                )
                
        return cte

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\parsers\__init__.py
================================================================================
"""
Parser package for SQL Converter with AST-based parsing.

This package provides SQL parsing capabilities using Abstract Syntax Trees (AST)
for more robust and accurate SQL transformations.
"""
from typing import Dict, Any, Optional, List, Type

# Import the main parser class
from sql_converter.parsers.sql_parser import SQLParser

# Map of supported dialects
# This maps our dialect names to internal implementation classes or modules
SUPPORTED_DIALECTS = {
    'ansi': 'ansi',
    'tsql': 'tsql',
    'mysql': 'mysql',
    'postgresql': 'postgres',
    'oracle': 'oracle',
    'snowflake': 'snowflake',
    'redshift': 'redshift',
    'spark': 'spark',
    'bigquery': 'bigquery',
}

# Default dialect to use if not specified
DEFAULT_DIALECT = 'ansi'


def create_parser(
    dialect: str = DEFAULT_DIALECT, 
    config: Optional[Dict[str, Any]] = None
) -> SQLParser:
    """
    Create a SQLParser instance for the specified dialect.
    
    Args:
        dialect: SQL dialect to use ('ansi', 'tsql', 'mysql', etc.)
        config: Additional configuration options for the parser
        
    Returns:
        SQLParser instance configured for the dialect
        
    Raises:
        ValueError: If the dialect is not supported
    """
    if dialect not in SUPPORTED_DIALECTS:
        # Fall back to default dialect with warning
        import logging
        logging.getLogger(__name__).warning(
            f"Unsupported dialect '{dialect}', falling back to '{DEFAULT_DIALECT}'"
        )
        dialect = DEFAULT_DIALECT
    
    # Create and return a parser instance
    return SQLParser(dialect=dialect, config=config)


def get_supported_dialects() -> List[str]:
    """
    Get a list of supported SQL dialects.
    
    Returns:
        List of dialect names supported by the parser
    """
    return list(SUPPORTED_DIALECTS.keys())


def is_dialect_supported(dialect: str) -> bool:
    """
    Check if a SQL dialect is supported.
    
    Args:
        dialect: Dialect name to check
        
    Returns:
        True if the dialect is supported, False otherwise
    """
    return dialect in SUPPORTED_DIALECTS


# Parse SQL string into AST
def parse_sql(sql: str, dialect: str = DEFAULT_DIALECT, **options) -> List[Any]:
    """
    Parse SQL string into AST expressions.
    
    This is a convenience function that creates a parser and parses the SQL.
    
    Args:
        sql: SQL string to parse
        dialect: SQL dialect to use
        **options: Additional parsing options
        
    Returns:
        List of AST expressions
        
    Raises:
        SQLSyntaxError: When SQL contains syntax errors
        ParserError: When parsing fails
    """
    parser = create_parser(dialect=dialect)
    return parser.parse(sql)


# Export public interface
__all__ = [
    'SQLParser',
    'create_parser',
    'get_supported_dialects',
    'is_dialect_supported',
    'parse_sql',
    'SUPPORTED_DIALECTS',
    'DEFAULT_DIALECT',
]

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\conftest.py
================================================================================
"""
Test fixtures and configuration for SQL Converter tests.
Updated to support AST-based parsing and conversion.
"""
import pytest
from pathlib import Path
import tempfile
import os
import sys

# Add path to root of project
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from sql_converter.utils.config import ConfigManager
from sql_converter.parsers.sql_parser import SQLParser
from sql_converter.converters.cte import CTEConverter
from sql_converter.cli import SQLConverterApp

@pytest.fixture
def config_manager():
    """Create a ConfigManager with test configuration."""
    manager = ConfigManager()
    manager.config = {
        'converters': ['cte'],
        'cte_converter': {
            'indent_spaces': 2,
            'temp_table_patterns': ['#.*']
        },
        'parser': {
            'dialect': 'ansi',
            'optimization_level': 0
        }
    }
    return manager

@pytest.fixture
def sql_parser():
    """Create a SQLParser instance for testing."""
    return SQLParser(dialect='ansi')

@pytest.fixture
def tsql_parser():
    """Create a T-SQL dialect parser for testing."""
    return SQLParser(dialect='tsql')

@pytest.fixture
def mysql_parser():
    """Create a MySQL dialect parser for testing."""
    return SQLParser(dialect='mysql')

@pytest.fixture
def cte_converter():
    """Create a CTEConverter instance with default config."""
    return CTEConverter()

@pytest.fixture
def configured_converter():
    """Create a CTEConverter with custom configuration."""
    config = {
        'indent_spaces': 2,
        'temp_table_patterns': ['#temp_.*', '#tmp_.*', '#.*']
    }
    return CTEConverter(config=config)

@pytest.fixture
def converter_app(config_manager, cte_converter):
    """Create a SQLConverterApp instance for testing."""
    return SQLConverterApp(
        converters={'cte': cte_converter},
        config=config_manager.config
    )

@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    with tempfile.TemporaryDirectory() as tmpdirname:
        yield Path(tmpdirname)

@pytest.fixture
def fixtures_path():
    """Return the path to test fixtures."""
    return Path(__file__).parent / 'fixtures'

@pytest.fixture
def sample_sql_file(temp_dir):
    """Create a sample SQL file for testing."""
    content = """
    -- Sample SQL with temp tables
    SELECT * INTO #temp FROM users WHERE status = 'active';
    SELECT name FROM #temp;
    """
    file_path = temp_dir / "test.sql"
    file_path.write_text(content)
    return file_path

@pytest.fixture
def complex_sql_file(temp_dir):
    """Create a more complex SQL file with multiple temp tables."""
    content = """
    SELECT * INTO #temp1 FROM users;
    SELECT * INTO #temp2 FROM orders;
    SELECT u.*, o.* FROM #temp1 u JOIN #temp2 o ON u.id = o.user_id;
    """
    file_path = temp_dir / "complex.sql"
    file_path.write_text(content)
    return file_path

@pytest.fixture
def load_fixture_sql():
    """
    Helper to load SQL from a fixture file.
    Usage: sql = load_fixture_sql('input/simple_select.sql')
    """
    fixtures_dir = Path(__file__).parent / 'fixtures'
    
    def _load(relative_path):
        path = fixtures_dir / relative_path
        if not path.exists():
            raise FileNotFoundError(f"Fixture not found: {relative_path}")
        return path.read_text()
        
    return _load

@pytest.fixture
def normalize_sql():
    """
    Helper to normalize SQL for comparison (removes whitespace differences).
    """
    import re
    
    def _normalize(sql):
        # Remove comments
        sql = re.sub(r'--.*?$', '', sql, flags=re.MULTILINE)
        # Replace newlines and multiple spaces with a single space
        sql = re.sub(r'\s+', ' ', sql)
        # Remove leading/trailing whitespace
        sql = sql.strip()
        return sql.lower()  # Case insensitive comparison
        
    return _normalize

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\expected\create_temp_table.sql
================================================================================
WITH orders_summary AS (
  SELECT customer_id, SUM(total) AS total_spent
  FROM orders
  GROUP BY customer_id
)
SELECT * FROM orders_summary WHERE total_spent > 1000;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\expected\multiple_temps.sql
================================================================================
WITH temp1 AS (
  SELECT * FROM table1
),
temp2 AS (
  SELECT * FROM table2
)
SELECT t1.*, t2.* FROM temp1 t1 JOIN temp2 t2 ON t1.id = t2.id;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\expected\nested_temps.sql
================================================================================
WITH inner_temp AS (
  SELECT * FROM source_table
),
outer_temp AS (
  SELECT * FROM inner_temp
)
SELECT * FROM outer_temp;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\expected\pattern_matching.sql
================================================================================
WITH tmp_users AS (
  SELECT * FROM users
)
SELECT * FROM tmp_users;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\expected\permanent_table.sql
================================================================================
SELECT * INTO permanent_table FROM users;
SELECT * FROM permanent_table;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\expected\simple_select.sql
================================================================================
WITH users_temp AS (
  SELECT * FROM users
)
SELECT * FROM users_temp;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\expected\special_chars.sql
================================================================================
WITH temp_123 AS (
  SELECT * FROM [table-with-hyphen]
)
SELECT * FROM temp_123;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\expected\tsql_brackets.sql
================================================================================
WITH temp AS (
  SELECT [col1] FROM [dbo].[table]
)
SELECT [col1] FROM temp;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\expected\with_comments.sql
================================================================================
WITH commented_temp AS (
  SELECT * FROM users /* important table */
)
SELECT * FROM commented_temp;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\expected\circular_deps.sql
================================================================================
-- This is expected to fail with a CircularDependencyError
-- No expected output is provided since the conversion should not complete

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\expected\complex_nesting.sql
================================================================================
WITH base_data AS (
  SELECT 
    id,
    name,
    email,
    status
  FROM 
    users
  WHERE 
    created_date > '2023-01-01'
),
user_metrics AS (
  SELECT
    user_id,
    COUNT(*) AS order_count,
    SUM(amount) AS total_spent,
    MAX(order_date) AS last_order_date
  FROM (
    SELECT
      o.id AS order_id,
      b.id AS user_id,
      o.amount,
      o.order_date
    FROM
      orders o
    JOIN
      base_data b ON o.user_id = b.id
    WHERE
      o.status = 'completed'
  ) order_data
  GROUP BY
    user_id
),
final_report AS (
  SELECT
    b.id,
    b.name,
    b.email,
    m.order_count,
    m.total_spent,
    m.last_order_date,
    ROW_NUMBER() OVER (PARTITION BY b.status ORDER BY m.total_spent DESC) AS spending_rank
  FROM
    base_data b
  LEFT JOIN
    user_metrics m ON b.id = m.user_id
),
high_value AS (
  SELECT * FROM final_report WHERE total_spent > 1000
),
recent_order AS (
  SELECT * FROM final_report WHERE last_order_date > '2023-06-01'
)
SELECT
  h.id,
  h.name,
  h.email,
  h.order_count,
  h.total_spent,
  CASE WHEN r.id IS NOT NULL THEN 'Yes' ELSE 'No' END AS recent_customer
FROM
  high_value h
LEFT JOIN
  recent_order r ON h.id = r.id
ORDER BY
  h.total_spent DESC;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\expected\existing_ctes.sql
================================================================================
WITH active_users AS (
  SELECT * FROM users WHERE status = 'active'
),
user_activity AS (
  SELECT
    au.id,
    au.name,
    au.email,
    COUNT(o.id) AS order_count
  FROM
    active_users au
  LEFT JOIN
    orders o ON au.id = o.user_id
  GROUP BY
    au.id, au.name, au.email
),
user_preferences AS (
  SELECT
    ua.id,
    ua.name,
    ua.order_count,
    p.preference_value
  FROM
    user_activity ua
  JOIN
    preferences p ON ua.id = p.user_id
  WHERE
    p.preference_type = 'email_frequency'
),
high_activity AS (
  SELECT * FROM user_activity WHERE order_count > 5
)
SELECT
  up.id,
  up.name,
  up.preference_value,
  ha.order_count
FROM
  user_preferences up
JOIN
  high_activity ha ON up.id = ha.id
ORDER BY
  ha.order_count DESC;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\expected\syntax_edge_cases.sql
================================================================================
WITH quoted_identifiers AS (
  SELECT 
    "column.with.dots", 
    "column with spaces",
    "column""with""quotes"
  FROM "schema.name"."table name"
),
oneliner AS (
  SELECT * FROM users
),
unicode AS (
  SELECT * FROM products WHERE category = N'家電製品' AND "説明書" IS NOT NULL
),
edge_case AS (
  SELECT 
    CASE 
      WHEN EXISTS (SELECT 1 FROM oneliner) THEN 'Exists' 
      ELSE 'Empty' 
    END AS result,
    CASE
      WHEN COUNT(*) > 0 THEN COUNT(*)
      ELSE NULL
    END AS record_count
  FROM quoted_identifiers
  WHERE 1 = 0
),
unions AS (
  SELECT * FROM (
    SELECT id, 'type1' AS source FROM quoted_identifiers
    UNION ALL
    SELECT id, 'type2' AS source FROM unicode
    UNION ALL
    SELECT NULL AS id, 'type3' AS source FROM edge_case
  ) combined_data
)
SELECT * FROM unions
ORDER BY COALESCE(id, 0), source;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\expected\multiple_dialects\mysql.sql
================================================================================
WITH temp_mysql AS (
  SELECT 
    `t`.`id`,
    `t`.`name`,
    COUNT(`o`.`order_id`) AS `order_count`,
    IFNULL(SUM(`o`.`amount`), 0) AS `total_amount`
  FROM 
    `users` AS `t`
  LEFT JOIN
    `orders` AS `o` ON `t`.`id` = `o`.`user_id`
  WHERE
    `t`.`created_at` > '2023-01-01'
  GROUP BY 
    `t`.`id`, `t`.`name`
  HAVING 
    `order_count` > 0
  ORDER BY 
    `total_amount` DESC
  LIMIT 100
)
SELECT 
  m.*,
  JSON_EXTRACT(`m`.`preferences`, '$.email_frequency') AS `email_pref`,
  CONCAT(`m`.`name`, ' (', SUBSTRING(`m`.`email`, 1, 10), '...)') AS `display_name`,
  DATEDIFF(NOW(), `m`.`created_at`) AS `days_active`
FROM 
  temp_mysql AS `m`
WHERE 
  `m`.`total_amount` > 500;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\expected\multiple_dialects\postgresql.sql
================================================================================
WITH pg_users AS (
  SELECT 
    t.id,
    t.name,
    t.email,
    t.created_at::date AS signup_date,
    COUNT(o.id) FILTER (WHERE o.status = 'completed') AS completed_orders,
    SUM(o.amount) FILTER (WHERE o.status = 'completed') AS total_spent
  FROM 
    users t
  LEFT JOIN
    orders o ON t.id = o.user_id
  WHERE
    t.created_at >= '2023-01-01'::date
  GROUP BY
    t.id, t.name, t.email, t.created_at
  HAVING
    COUNT(o.id) > 0
  ORDER BY
    total_spent DESC NULLS LAST
  LIMIT 100 OFFSET 0
),
recent_orders AS (
  SELECT 
    user_id,
    jsonb_agg(
      jsonb_build_object(
        'order_id', id,
        'date', created_at,
        'amount', amount
      )
    ) AS orders_json
  FROM
    orders
  WHERE
    created_at > now() - interval '30 days'
  GROUP BY
    user_id
)
SELECT
  u.*,
  COALESCE(ro.orders_json, '[]'::jsonb) AS recent_orders,
  CASE 
    WHEN EXTRACT(YEAR FROM AGE(NOW(), u.created_at::timestamp)) > 1 
    THEN 'loyal' 
    ELSE 'new' 
  END AS user_type
FROM
  pg_users u
LEFT JOIN
  recent_orders ro ON u.id = ro.user_id
WHERE
  u.completed_orders > 2;


################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\expected\multiple_dialects\tsql.sql
================================================================================
WITH tsql_users AS (
  SELECT 
    [u].[UserId],
    [u].[UserName],
    [u].[Email],
    CONVERT(DATE, [u].[CreatedAt]) AS [SignupDate],
    COUNT([o].[OrderId]) AS [OrderCount],
    SUM([o].[Amount]) AS [TotalSpent]
  FROM 
    [dbo].[Users] [u]
  LEFT JOIN
    [dbo].[Orders] [o] ON [u].[UserId] = [o].[UserId]
  WHERE
    [u].[CreatedAt] >= '20230101'
    AND [u].[IsActive] = 1
  GROUP BY
    [u].[UserId], [u].[UserName], [u].[Email], [u].[CreatedAt]
  HAVING
    COUNT([o].[OrderId]) > 0
  ORDER BY
    [TotalSpent] DESC
)
SELECT
  [u].*,
  [p].[PhoneNumber],
  [r].[RegionName],
  STUFF([u].[Email], 2, CHARINDEX('@', [u].[Email]) - 2, '***') AS [MaskedEmail],
  CASE 
    WHEN [u].[TotalSpent] > 1000 THEN 'High'
    WHEN [u].[TotalSpent] > 500 THEN 'Medium'
    ELSE 'Low'
  END AS [SpendingCategory],
  DATEDIFF(DAY, CONVERT(DATE, [u].[SignupDate]), GETDATE()) AS [DaysSinceSignup]
FROM
  tsql_users [u]
LEFT JOIN
  [dbo].[UserProfiles] [p] ON [u].[UserId] = [p].[UserId]
LEFT JOIN
  [dbo].[Regions] [r] ON [p].[RegionId] = [r].[RegionId]
WHERE
  [u].[OrderCount] > 2
  AND EXISTS (
    SELECT 1 FROM [dbo].[Orders] [o]
    WHERE [o].[UserId] = [u].[UserId]
    AND [o].[OrderDate] > @cutoff_date
  )
ORDER BY
  [u].[TotalSpent] DESC
OFFSET 0 ROWS FETCH NEXT 100 ROWS ONLY;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\input\circular_deps.sql
================================================================================
-- This file is used to test circular dependency detection
-- and should cause a validation error when processed

-- First temporary table depends on the second (not yet defined)
SELECT * INTO #temp1 FROM #temp2;

-- Second temporary table depends on the first (creating a circular dependency)
SELECT * INTO #temp2 FROM #temp1;

-- Query using both tables
SELECT 
    t1.id,
    t2.name
FROM 
    #temp1 t1
JOIN
    #temp2 t2 ON t1.id = t2.id;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\input\complex_nesting.sql
================================================================================
-- This file tests complex nested queries with multiple levels of temp tables

-- Create base temp table from subquery
SELECT * INTO #base_data FROM (
    SELECT 
        id,
        name,
        email,
        status
    FROM 
        users
    WHERE 
        created_date > '2023-01-01'
) recent_users;

-- Create intermediate temp table with aggregation from base table
SELECT
    user_id,
    COUNT(*) AS order_count,
    SUM(amount) AS total_spent,
    MAX(order_date) AS last_order_date
INTO #user_metrics
FROM (
    SELECT
        o.id AS order_id,
        b.id AS user_id,
        o.amount,
        o.order_date
    FROM
        orders o
    JOIN
        #base_data b ON o.user_id = b.id
    WHERE
        o.status = 'completed'
) order_data
GROUP BY
    user_id;

-- Create final temp table joining multiple sources with window functions
SELECT
    b.id,
    b.name,
    b.email,
    m.order_count,
    m.total_spent,
    m.last_order_date,
    ROW_NUMBER() OVER (PARTITION BY b.status ORDER BY m.total_spent DESC) AS spending_rank
INTO #final_report
FROM
    #base_data b
LEFT JOIN
    #user_metrics m ON b.id = m.user_id;

-- Final query with nested CTEs and multiple joins
WITH high_value AS (
    SELECT * FROM #final_report WHERE total_spent > 1000
),
recent_order AS (
    SELECT * FROM #final_report WHERE last_order_date > '2023-06-01'
)
SELECT
    h.id,
    h.name,
    h.email,
    h.order_count,
    h.total_spent,
    CASE WHEN r.id IS NOT NULL THEN 'Yes' ELSE 'No' END AS recent_customer
FROM
    high_value h
LEFT JOIN
    recent_order r ON h.id = r.id
ORDER BY
    h.total_spent DESC;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\input\existing_ctes.sql
================================================================================
-- Test handling SQL with existing CTEs and temp tables mixed together

-- Start with a CTE for active users
WITH active_users AS (
    SELECT * FROM users WHERE status = 'active'
)
-- Create a temp table from the CTE
SELECT
    au.id,
    au.name,
    au.email,
    COUNT(o.id) AS order_count
INTO #user_activity
FROM
    active_users au
LEFT JOIN
    orders o ON au.id = o.user_id
GROUP BY
    au.id, au.name, au.email;

-- Create another temp table from the first temp table
SELECT
    ua.id,
    ua.name,
    ua.order_count,
    p.preference_value
INTO #user_preferences
FROM
    #user_activity ua
JOIN
    preferences p ON ua.id = p.user_id
WHERE
    p.preference_type = 'email_frequency';

-- Final query using all CTEs and temp tables
WITH high_activity AS (
    SELECT * FROM #user_activity WHERE order_count > 5
)
SELECT
    up.id,
    up.name,
    up.preference_value,
    ha.order_count
FROM
    #user_preferences up
JOIN
    high_activity ha ON up.id = ha.id
ORDER BY
    ha.order_count DESC;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\input\syntax_edge_cases.sql
================================================================================
-- Test handling of various SQL syntax edge cases

-- Edge case 1: Quoted identifiers with special characters
SELECT 
    "column.with.dots", 
    "column with spaces",
    "column""with""quotes"
INTO #quoted_identifiers
FROM "schema.name"."table name";

-- Edge case 2: Multiple statements on one line with comments
SELECT * INTO #oneliner FROM users; /* inline comment */ SELECT * FROM #oneliner;

-- Edge case 3: Unicode characters in identifiers and strings
SELECT * INTO #unicode FROM products WHERE category = N'家電製品' AND "説明書" IS NOT NULL;

-- Edge case 4: Empty result set with complex CASE expression
SELECT 
    CASE 
        WHEN EXISTS (SELECT 1 FROM #oneliner) THEN 'Exists' 
        ELSE 'Empty' 
    END AS result,
    CASE
        WHEN COUNT(*) > 0 THEN COUNT(*)
        ELSE NULL
    END AS record_count
INTO #edge_case
FROM #quoted_identifiers
WHERE 1 = 0;

-- Edge case 5: Nested UNION and temp table references
SELECT * INTO #unions FROM (
    SELECT id, 'type1' AS source FROM #quoted_identifiers
    UNION ALL
    SELECT id, 'type2' AS source FROM #unicode
    UNION ALL
    SELECT NULL AS id, 'type3' AS source FROM #edge_case
) combined_data;

-- Final query using all temp tables
SELECT * FROM #unions
ORDER BY COALESCE(id, 0), source;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\input\multiple_dialects\mysql.sql
================================================================================
-- MySQL dialect-specific features test

-- Using MySQL-specific syntax with backtick identifiers
CREATE TEMPORARY TABLE #temp_mysql AS
SELECT 
    `t`.`id`,
    `t`.`name`,
    COUNT(`o`.`order_id`) AS `order_count`,
    IFNULL(SUM(`o`.`amount`), 0) AS `total_amount`
FROM 
    `users` AS `t`
LEFT JOIN
    `orders` AS `o` ON `t`.`id` = `o`.`user_id`
WHERE
    `t`.`created_at` > '2023-01-01'
GROUP BY 
    `t`.`id`, `t`.`name`
HAVING 
    `order_count` > 0
ORDER BY 
    `total_amount` DESC
LIMIT 100;

-- Second query with more MySQL features
SELECT 
    m.*,
    JSON_EXTRACT(`m`.`preferences`, '$.email_frequency') AS `email_pref`,
    CONCAT(`m`.`name`, ' (', SUBSTRING(`m`.`email`, 1, 10), '...)') AS `display_name`,
    DATEDIFF(NOW(), `m`.`created_at`) AS `days_active`
FROM 
    #temp_mysql AS `m`
WHERE 
    `m`.`total_amount` > 500;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\input\multiple_dialects\postgresql.sql
================================================================================
-- PostgreSQL dialect-specific features test

-- Using PostgreSQL-specific syntax
SELECT 
    t.id,
    t.name,
    t.email,
    t.created_at::date AS signup_date,
    COUNT(o.id) FILTER (WHERE o.status = 'completed') AS completed_orders,
    SUM(o.amount) FILTER (WHERE o.status = 'completed') AS total_spent
INTO TEMP #pg_users
FROM 
    users t
LEFT JOIN
    orders o ON t.id = o.user_id
WHERE
    t.created_at >= '2023-01-01'::date
GROUP BY
    t.id, t.name, t.email, t.created_at
HAVING
    COUNT(o.id) > 0
ORDER BY
    total_spent DESC NULLS LAST
LIMIT 100 OFFSET 0;

-- Query with more PostgreSQL features
WITH recent_orders AS (
    SELECT 
        user_id,
        jsonb_agg(
            jsonb_build_object(
                'order_id', id,
                'date', created_at,
                'amount', amount
            )
        ) AS orders_json
    FROM
        orders
    WHERE
        created_at > now() - interval '30 days'
    GROUP BY
        user_id
)
SELECT
    u.*,
    COALESCE(ro.orders_json, '[]'::jsonb) AS recent_orders,
    CASE 
        WHEN EXTRACT(YEAR FROM AGE(NOW(), u.created_at::timestamp)) > 1 
        THEN 'loyal' 
        ELSE 'new' 
    END AS user_type
FROM
    #pg_users u
LEFT JOIN
    recent_orders ro ON u.id = ro.user_id
WHERE
    u.completed_orders > 2;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\extended\input\multiple_dialects\tsql.sql
================================================================================
-- T-SQL dialect-specific features test

-- Using T-SQL specific syntax with bracket identifiers
SELECT 
    [u].[UserId],
    [u].[UserName],
    [u].[Email],
    CONVERT(DATE, [u].[CreatedAt]) AS [SignupDate],
    COUNT([o].[OrderId]) AS [OrderCount],
    SUM([o].[Amount]) AS [TotalSpent]
INTO #tsql_users
FROM 
    [dbo].[Users] [u]
LEFT JOIN
    [dbo].[Orders] [o] ON [u].[UserId] = [o].[UserId]
WHERE
    [u].[CreatedAt] >= '20230101' -- YYYYMMDD format
    AND [u].[IsActive] = 1
GROUP BY
    [u].[UserId], [u].[UserName], [u].[Email], [u].[CreatedAt]
HAVING
    COUNT([o].[OrderId]) > 0
ORDER BY
    [TotalSpent] DESC;

-- Query with more T-SQL features
DECLARE @cutoff_date DATETIME = DATEADD(MONTH, -3, GETDATE())

SELECT
    [u].*,
    [p].[PhoneNumber],
    [r].[RegionName],
    STUFF([u].[Email], 2, CHARINDEX('@', [u].[Email]) - 2, '***') AS [MaskedEmail],
    CASE 
        WHEN [u].[TotalSpent] > 1000 THEN 'High'
        WHEN [u].[TotalSpent] > 500 THEN 'Medium'
        ELSE 'Low'
    END AS [SpendingCategory],
    DATEDIFF(DAY, CONVERT(DATE, [u].[SignupDate]), GETDATE()) AS [DaysSinceSignup]
FROM
    #tsql_users [u]
LEFT JOIN
    [dbo].[UserProfiles] [p] ON [u].[UserId] = [p].[UserId]
LEFT JOIN
    [dbo].[Regions] [r] ON [p].[RegionId] = [r].[RegionId]
WHERE
    [u].[OrderCount] > 2
    AND EXISTS (
        SELECT 1 FROM [dbo].[Orders] [o]
        WHERE [o].[UserId] = [u].[UserId]
        AND [o].[OrderDate] > @cutoff_date
    )
ORDER BY
    [u].[TotalSpent] DESC
OFFSET 0 ROWS FETCH NEXT 100 ROWS ONLY;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\input\create_temp_table.sql
================================================================================
CREATE TEMP TABLE #orders_summary AS
SELECT customer_id, SUM(total) AS total_spent
FROM orders
GROUP BY customer_id;

SELECT * FROM #orders_summary WHERE total_spent > 1000;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\input\multiple_temps.sql
================================================================================
SELECT * INTO #temp1 FROM table1;
SELECT * INTO #temp2 FROM table2;
SELECT t1.*, t2.* FROM #temp1 t1 JOIN #temp2 t2 ON t1.id = t2.id;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\input\nested_temps.sql
================================================================================
SELECT * INTO #inner_temp FROM source_table;
SELECT * INTO #outer_temp FROM #inner_temp;
SELECT * FROM #outer_temp;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\input\pattern_matching.sql
================================================================================
SELECT * INTO #tmp_users FROM users;
SELECT * FROM #tmp_users;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\input\permanent_table.sql
================================================================================
SELECT * INTO permanent_table FROM users;
SELECT * FROM permanent_table;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\input\simple_select.sql
================================================================================
SELECT * INTO #users_temp FROM users;
SELECT * FROM #users_temp;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\input\special_chars.sql
================================================================================
SELECT * INTO #temp_123 FROM [table-with-hyphen];
SELECT * FROM #temp_123;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\input\tsql_brackets.sql
================================================================================
SELECT [col1] INTO #temp FROM [dbo].[table];
SELECT [col1] FROM #temp;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\fixtures\input\with_comments.sql
================================================================================
-- Create temp table
SELECT * INTO #commented_temp FROM users /* important table */;
/*
Multi-line comment
SELECT * INTO #ignored_temp FROM logs;
*/
SELECT * FROM #commented_temp;

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\functional\test_dependency_resolution.py
================================================================================
"""
Tests for dependency resolution between temporary tables.
Tests the ordering and dependency handling in CTEConverter.
"""
import pytest
from sqlglot import exp

from sql_converter.parsers.sql_parser import SQLParser
from sql_converter.converters.cte import CTEConverter
from sql_converter.exceptions import ValidationError, CircularDependencyError


class TestDependencyResolution:
    """Test suite for temporary table dependency resolution."""
    
    @pytest.fixture
    def parser(self):
        """Create a SQLParser instance."""
        return SQLParser()
    
    @pytest.fixture
    def converter(self):
        """Create a CTEConverter instance."""
        return CTEConverter()
    
    def test_simple_dependency(self, parser, converter):
        """Test simple dependency between two temp tables."""
        sql = """
        -- First temp table
        SELECT * INTO #temp1 FROM users WHERE status = 'active';
        
        -- Second temp table depends on first
        SELECT * INTO #temp2 FROM #temp1 WHERE registration_date > '2023-01-01';
        
        -- Query uses both temp tables
        SELECT 
            t1.id, 
            t1.name,
            t2.registration_date
        FROM 
            #temp1 t1
        LEFT JOIN
            #temp2 t2 ON t1.id = t2.id;
        """
        
        # Parse the SQL into AST
        expressions = parser.parse(sql)
        
        # Convert using AST method
        converted = converter.convert_ast(expressions, parser)
        
        # Convert back to SQL
        result = parser.to_sql(converted[0])
        
        # Verify dependency order in the result
        # #temp1 should be defined before #temp2
        temp1_pos = result.find("temp1 AS")
        temp2_pos = result.find("temp2 AS")
        
        assert temp1_pos >= 0
        assert temp2_pos >= 0
        assert temp1_pos < temp2_pos  # temp1 should be defined first

    def test_multiple_dependencies(self, parser, converter):
        """Test with multiple dependencies between temp tables."""
        sql = """
        -- Base temp tables
        SELECT * INTO #customers FROM users WHERE type = 'customer';
        SELECT * INTO #products FROM items WHERE category = 'product';
        
        -- Intermediate temp table depends on both
        SELECT 
            c.id AS customer_id,
            p.id AS product_id,
            o.order_date
        INTO #orders 
        FROM orders o
        JOIN #customers c ON o.customer_id = c.id
        JOIN #products p ON o.product_id = p.id;
        
        -- Final temp table depends on intermediate
        SELECT
            customer_id,
            COUNT(product_id) AS product_count,
            MAX(order_date) AS latest_order
        INTO #customer_stats
        FROM #orders
        GROUP BY customer_id;
        
        -- Final query
        SELECT 
            c.name,
            c.email,
            s.product_count,
            s.latest_order
        FROM 
            #customers c
        JOIN
            #customer_stats s ON c.id = s.customer_id
        ORDER BY
            s.product_count DESC;
        """
        
        # Parse the SQL into AST
        expressions = parser.parse(sql)
        
        # Convert using AST method
        converted = converter.convert_ast(expressions, parser)
        
        # Convert back to SQL
        result = parser.to_sql(converted[0])
        
        # Verify dependency order in the result
        # This order should be followed:
        # 1. #customers and #products (no dependencies)
        # 2. #orders (depends on #customers and #products)
        # 3. #customer_stats (depends on #orders)
        customers_pos = result.find("customers AS")
        products_pos = result.find("products AS")
        orders_pos = result.find("orders AS")
        stats_pos = result.find("customer_stats AS")
        
        # Base tables should come before tables that depend on them
        assert min(customers_pos, products_pos) < orders_pos
        assert orders_pos < stats_pos

    def test_circular_dependency_detection(self, parser, converter):
        """Test detection of circular dependencies."""
        sql = """
        -- Circular dependency between temp tables
        SELECT * INTO #temp1 FROM #temp2;
        SELECT * INTO #temp2 FROM #temp1;
        
        SELECT * FROM #temp1;
        """
        
        # Parse the SQL into AST
        expressions = parser.parse(sql)
        
        # This should raise a validation error
        with pytest.raises((ValidationError, CircularDependencyError)) as excinfo:
            converter.convert_ast(expressions, parser)
            
        # Error should mention circular dependency
        assert "circular" in str(excinfo.value).lower()
        assert "dependency" in str(excinfo.value).lower()

    def test_self_reference_handling(self, parser, converter):
        """Test handling of self-references in temp tables."""
        sql = """
        -- Create initial temp table
        SELECT * INTO #temp FROM users;
        
        -- Update the temp table by self-reference
        -- (This is normally invalid in SQL, but testing how the parser handles it)
        SELECT * INTO #temp FROM #temp WHERE status = 'active';
        
        SELECT * FROM #temp;
        """
        
        # Parse the SQL into AST
        expressions = parser.parse(sql)
        
        # This might raise an error, or produce a specific result
        # We're just testing that it's handled consistently
        try:
            converted = converter.convert_ast(expressions, parser)
            
            # If no error, verify the output makes sense
            result = parser.to_sql(converted[0])
            
            # We should still have a temp definition
            assert "WITH temp AS" in result
            
        except ValidationError as e:
            # If it raises an error for invalid SQL, that's fine too
            assert "reference" in str(e).lower() or "dependency" in str(e).lower()

    def test_complex_dependency_graph(self, parser, converter):
        """Test complex dependency graph with multiple paths."""
        sql = """
        -- Base tables
        SELECT * INTO #A FROM table1;
        SELECT * INTO #B FROM table2;
        
        -- Mid-level dependencies
        SELECT * INTO #C FROM #A;
        SELECT * INTO #D FROM #A JOIN #B ON #A.id = #B.id;
        SELECT * INTO #E FROM #B;
        
        -- Top-level dependencies
        SELECT * INTO #F FROM #C JOIN #D ON #C.id = #D.id;
        SELECT * INTO #G FROM #D JOIN #E ON #D.id = #E.id;
        
        -- Final query
        SELECT * 
        FROM #F f
        JOIN #G g ON f.id = g.id;
        """
        
        # Parse the SQL into AST
        expressions = parser.parse(sql)
        
        # Convert using AST method
        converted = converter.convert_ast(expressions, parser)
        
        # Convert back to SQL
        result = parser.to_sql(converted[0])
        
        # Verify dependency order in the result
        # Dependency graph: 
        # Level 1: A, B
        # Level 2: C (depends on A), D (depends on A,B), E (depends on B)
        # Level 3: F (depends on C,D), G (depends on D,E)
        # 
        # These conditions should hold:
        # - A and B before C, D, E
        # - C, D, E before F, G
        
        pos = {name: result.find(f"{name.lower()} AS") for name in ['A', 'B', 'C', 'D', 'E', 'F', 'G']}
        
        # Level 1 before Level 2
        assert max(pos['A'], pos['B']) < min(pos['C'], pos['D'], pos['E'])
        
        # Level 2 before Level 3
        assert max(pos['C'], pos['D'], pos['E']) < min(pos['F'], pos['G'])
        
        # Dependent tables must come after all their dependencies
        assert pos['C'] > pos['A']  # C depends on A
        assert pos['D'] > max(pos['A'], pos['B'])  # D depends on A and B
        assert pos['E'] > pos['B']  # E depends on B
        assert pos['F'] > max(pos['C'], pos['D'])  # F depends on C and D
        assert pos['G'] > max(pos['D'], pos['E'])  # G depends on D and E

    def test_dependency_extraction(self, parser, converter):
        """Test the correct extraction of dependencies from SQL."""
        sql = """
        SELECT * INTO #temp1 FROM users;
        
        -- This has a dependency in a subquery
        SELECT * INTO #temp2 FROM (
            SELECT t1.*, p.profile_data 
            FROM #temp1 t1
            JOIN profiles p ON t1.id = p.user_id
        ) enriched_data;
        
        -- This has a dependency in a JOIN
        SELECT * INTO #temp3 
        FROM orders o
        JOIN #temp1 t1 ON o.user_id = t1.id
        JOIN #temp2 t2 ON o.user_id = t2.id;
        
        SELECT * FROM #temp3;
        """
        
        # Parse the SQL into AST
        expressions = parser.parse(sql)
        
        # Convert using AST method
        converted = converter.convert_ast(expressions, parser)
        
        # Convert back to SQL
        result = parser.to_sql(converted[0])
        
        # Verify the dependencies were correctly extracted and ordered
        temp1_pos = result.find("temp1 AS")
        temp2_pos = result.find("temp2 AS")
        temp3_pos = result.find("temp3 AS")
        
        # Check the ordering
        assert temp1_pos < temp2_pos < temp3_pos

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\functional\test_dialects.py
================================================================================
"""
Functional tests for SQL dialect handling.
Tests dialect-specific features in the parser and converter.
"""
import pytest
from sqlglot import exp

from sql_converter.parsers.sql_parser import SQLParser
from sql_converter.converters.cte import CTEConverter
from sql_converter.exceptions import DialectError


class TestDialectHandling:
    """Test suite for SQL dialect handling."""
    
    @pytest.fixture
    def dialect_parsers(self):
        """Create parsers for different SQL dialects."""
        return {
            'ansi': SQLParser(dialect='ansi'),
            'tsql': SQLParser(dialect='tsql'),
            'mysql': SQLParser(dialect='mysql'),
            'postgresql': SQLParser(dialect='postgresql'),
        }
    
    @pytest.fixture
    def converter(self):
        """Create a CTEConverter instance."""
        return CTEConverter()
    
    def test_tsql_specific_features(self, dialect_parsers, converter):
        """Test T-SQL specific features like bracket identifiers."""
        tsql_parser = dialect_parsers['tsql']
        
        # T-SQL with bracket identifiers and schema references
        sql = """
        SELECT [Column Name], [Order.Date]
        INTO #temp_table
        FROM [dbo].[TableName]
        WHERE [Column Name] LIKE N'%test%';
        
        SELECT * FROM #temp_table
        ORDER BY [Order.Date];
        """
        
        # Parse with T-SQL parser
        expressions = tsql_parser.parse(sql)
        
        # Verify bracket identifiers are preserved
        select_expr = expressions[0]
        select_clause = select_expr.find_all(exp.Column)
        column_names = [col.alias_or_name for col in select_clause]
        assert 'Column Name' in column_names
        assert 'Order.Date' in column_names
        
        # Verify schema notation is preserved
        table = select_expr.find(exp.Table)
        assert table.db == 'dbo'
        assert table.name == 'TableName'
        
        # Convert using AST API
        converted = converter.convert_ast(expressions, tsql_parser)
        
        # Convert back to SQL
        result = tsql_parser.to_sql(converted[0])
        
        # Verify T-SQL specific features are preserved
        assert "WITH temp_table AS" in result
        assert "[Column Name]" in result
        assert "[Order.Date]" in result
        assert "[dbo].[TableName]" in result
        assert "ORDER BY [Order.Date]" in result

    def test_mysql_specific_features(self, dialect_parsers, converter):
        """Test MySQL specific features like backtick identifiers."""
        mysql_parser = dialect_parsers['mysql']
        
        # MySQL with backtick identifiers and LIMIT
        sql = """
        CREATE TEMPORARY TABLE #temp_table AS
        SELECT `id`, `user.name` AS `name`
        FROM `users`
        WHERE `status` = 'active'
        LIMIT 100;
        
        SELECT * FROM #temp_table;
        """
        
        # Parse with MySQL parser
        expressions = mysql_parser.parse(sql)
        
        # Verify MySQL features
        assert any(expr.find(exp.Limit) for expr in expressions)
        
        # Convert using AST API
        converted = converter.convert_ast(expressions, mysql_parser)
        
        # Convert back to SQL
        result = mysql_parser.to_sql(converted[0])
        
        # Verify MySQL specific features are preserved
        assert "WITH temp_table AS" in result
        assert "`user.name`" in result or "`name`" in result
        assert "`users`" in result
        assert "LIMIT 100" in result

    def test_postgresql_specific_features(self, dialect_parsers, converter):
        """Test PostgreSQL specific features."""
        pg_parser = dialect_parsers['postgresql']
        
        # PostgreSQL with double-quoted identifiers and specific functions
        sql = """
        SELECT * INTO #temp_table FROM "public"."users"
        WHERE "registration_date"::date > '2023-01-01';
        
        SELECT * FROM #temp_table
        ORDER BY "last_login" DESC NULLS LAST;
        """
        
        # Parse with PostgreSQL parser
        expressions = pg_parser.parse(sql)
        
        # Convert using AST API
        converted = converter.convert_ast(expressions, pg_parser)
        
        # Convert back to SQL
        result = pg_parser.to_sql(converted[0])
        
        # Verify PostgreSQL specific features are preserved
        assert "WITH temp_table AS" in result
        assert '"public"."users"' in result or 'public.users' in result
        assert "::date" in result.lower() or "CAST" in result.upper()

    def test_cross_dialect_conversion(self, dialect_parsers, converter):
        """Test converting SQL from one dialect to another."""
        # Original SQL in T-SQL
        tsql = """
        SELECT [id], [name] INTO #temp FROM [dbo].[users];
        SELECT * FROM #temp WHERE [id] > 100;
        """
        
        # Parse with T-SQL parser
        tsql_parser = dialect_parsers['tsql']
        tsql_expressions = tsql_parser.parse(tsql)
        
        # Convert to PostgreSQL
        pg_parser = dialect_parsers['postgresql']
        
        # Convert the AST first
        converted_ast = converter.convert_ast(tsql_expressions, tsql_parser)
        
        # Generate PostgreSQL SQL from the AST
        # Note: In a real implementation, you would need a dialect conversion step
        # This is a simplified example
        pg_sql = pg_parser.to_sql(converted_ast[0])
        
        # Verify the basic structure is preserved
        assert "WITH temp AS" in pg_sql
        # PostgreSQL would use double quotes instead of brackets
        assert "id" in pg_sql
        assert "name" in pg_sql

    def test_dialect_specific_error_handling(self, dialect_parsers):
        """Test that dialect-specific errors are properly handled."""
        # SQL with PostgreSQL specific syntax
        pg_specific_sql = """
        SELECT * INTO #temp FROM users
        WHERE created_at::date > '2023-01-01';
        """
        
        # This should parse fine with PostgreSQL parser
        pg_parser = dialect_parsers['postgresql']
        pg_parser.parse(pg_specific_sql)
        
        # But may fail with other dialects that don't support :: cast syntax
        tsql_parser = dialect_parsers['tsql']
        try:
            tsql_parser.parse(pg_specific_sql)
        except Exception as e:
            # This is expected - either we get a dialect error or syntax error
            assert True
            return
            
        # If we reach here without exception, verify that the parser handled the syntax difference
        # by checking the output SQL doesn't contain the ::date cast
        expressions = tsql_parser.parse(pg_specific_sql)
        result = tsql_parser.to_sql(expressions[0])
        assert "::date" not in result

    def test_dialect_detection(self, dialect_parsers):
        """Test automatic dialect detection (if supported)."""
        # Note: This test would depend on whether your implementation supports
        # automatic dialect detection. If not, this could be a placeholder for future functionality.
        
        # Sample SQL with dialect-specific features
        sql = "SELECT [Column] FROM [Table];"  # T-SQL style
        
        # A hypothetical function that could detect the dialect
        # If your implementation doesn't have this, you can skip this test
        try:
            from sql_converter.parsers.sql_parser import detect_dialect
            detected = detect_dialect(sql)
            assert detected == 'tsql'
        except ImportError:
            # Function doesn't exist yet, mark as skipped
            pytest.skip("Dialect detection not implemented")
            
    def test_dialect_conversion(self, dialect_parsers, converter):
        """Test conversion between SQL dialects."""
        # This test verifies that the parser can convert SQL between dialects
        # This might require additional functionality to be implemented
        
        tsql_parser = dialect_parsers['tsql']
        ansi_parser = dialect_parsers['ansi']
        
        # T-SQL specific SQL
        tsql = """
        SELECT [Column1], [Column2] INTO #temp FROM [dbo].[Table]
        WHERE [Column1] LIKE N'%test%';
        """
        
        # Parse with T-SQL parser
        tsql_expressions = tsql_parser.parse(tsql)
        
        # Convert the temporary table
        converted_ast = converter.convert_ast(tsql_expressions, tsql_parser)
        
        # Try to convert to ANSI SQL format
        try:
            ansi_sql = ansi_parser.to_sql(converted_ast[0])
            
            # Verify the conversion looks like ANSI SQL (no brackets)
            assert "[" not in ansi_sql
            assert "]" not in ansi_sql
            assert "WITH temp AS" in ansi_sql
            assert "Column1" in ansi_sql
            assert "Column2" in ansi_sql
        except Exception as e:
            # If this functionality is not implemented, skip the test
            pytest.skip("Cross-dialect conversion not fully implemented")

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\functional\test_error_handling.py
================================================================================
"""
Tests for error handling in the AST-based SQL parser and converter.
Focuses on verifying clear and helpful error messages.
"""
import pytest
import re

from sql_converter.parsers.sql_parser import SQLParser
from sql_converter.converters.cte import CTEConverter
from sql_converter.exceptions import (
    SQLSyntaxError, ParserError, CircularDependencyError, 
    ValidationError, DialectError, ASTError
)


class TestErrorHandling:
    """Test suite for error handling in AST-based parsing and conversion."""
    
    @pytest.fixture
    def parser(self):
        """Create a SQLParser instance."""
        return SQLParser()
    
    @pytest.fixture
    def converter(self):
        """Create a CTEConverter instance."""
        return CTEConverter()
    
    def test_syntax_error_reporting(self, parser):
        """Test that syntax errors are reported with useful information."""
        # SQL with syntax errors
        invalid_sql_samples = [
            # Missing FROM clause
            "SELECT id, name WHERE status = 'active';",
            
            # Unbalanced parentheses
            "SELECT * FROM (SELECT id FROM users WHERE id > 100;",
            
            # Invalid JOIN syntax
            "SELECT * FROM users JOIN orders;",
            
            # Invalid GROUP BY position
            "SELECT * FROM users GROUP BY name WHERE status = 'active';"
        ]
        
        for invalid_sql in invalid_sql_samples:
            # Should raise a SQLSyntaxError
            with pytest.raises(SQLSyntaxError) as excinfo:
                parser.parse(invalid_sql)
            
            # Error message should be clear and helpful
            error_msg = str(excinfo.value).lower()
            assert "syntax error" in error_msg
            
            # Should either have line/position info or a clear description
            has_position = re.search(r'(at line|at position)', error_msg) is not None
            has_description = any(term in error_msg for term in ['missing', 'invalid', 'expected', 'unbalanced'])
            
            assert has_position or has_description, f"Error message lacks position or description: {error_msg}"

    def test_validation_error_reporting(self, parser, converter):
        """Test validation errors are reported clearly."""
        # SQL with circular dependency
        circular_sql = """
        SELECT * INTO #temp1 FROM #temp2;
        SELECT * INTO #temp2 FROM #temp1;
        """
        
        # Should raise a ValidationError or CircularDependencyError
        with pytest.raises((ValidationError, CircularDependencyError)) as excinfo:
            expressions = parser.parse(circular_sql)
            converter.convert_ast(expressions, parser)
            
        # Error message should mention circular dependency
        error_msg = str(excinfo.value).lower()
        assert "circular" in error_msg
        assert "dependency" in error_msg
        
        # Should mention the tables involved
        assert "#temp1" in error_msg or "temp1" in error_msg
        assert "#temp2" in error_msg or "temp2" in error_msg

    def test_empty_sql_handling(self, parser):
        """Test handling of empty SQL input."""
        # Empty SQL should be rejected with a clear error
        with pytest.raises(SQLSyntaxError) as excinfo:
            parser.parse("")
            
        # Error should indicate empty input
        assert "empty sql statement" in str(excinfo.value).lower()
        
        # Just whitespace should also be rejected
        with pytest.raises(SQLSyntaxError):
            parser.parse("   \n   ")

    def test_dialect_specific_errors(self):
        """Test errors for dialect-specific features used with wrong dialect."""
        # T-SQL specific syntax
        tsql = "SELECT [Column Name] FROM [dbo].[Table];"
        
        # Parse with a MySQL parser (which doesn't support bracket identifiers)
        mysql_parser = SQLParser(dialect='mysql')
        
        # This might raise an error or silently handle it
        # We're just ensuring it doesn't crash unexpectedly
        try:
            mysql_parser.parse(tsql)
        except (SQLSyntaxError, DialectError):
            # Expected behavior - dialect error
            pass
        except Exception as e:
            # Unexpected error type
            pytest.fail(f"Unexpected error type: {type(e).__name__}: {str(e)}")

    def test_helpful_error_messages(self, parser, converter):
        """Test that error messages are helpful and actionable."""
        # Various problematic SQL inputs
        test_cases = [
            # Invalid join condition
            {
                'sql': "SELECT * FROM users u JOIN orders o;",
                'expected_terms': ['join', 'missing', 'condition', 'on']
            },
            
            # Unmatched quotes
            {
                'sql': "SELECT * FROM users WHERE name = 'John;",
                'expected_terms': ['quote', 'unbalanced', 'unmatched']
            },
            
            # Invalid table name
            {
                'sql': "SELECT * FROM ;",
                'expected_terms': ['table', 'missing', 'name']
            }
        ]
        
        for case in test_cases:
            with pytest.raises(SQLSyntaxError) as excinfo:
                parser.parse(case['sql'])
                
            # Error message should contain at least one expected term
            error_msg = str(excinfo.value).lower()
            assert any(term in error_msg for term in case['expected_terms']), \
                f"Error message '{error_msg}' should contain one of {case['expected_terms']}"

    def test_error_location_information(self, parser):
        """Test that error messages include location information when possible."""
        # SQL with a syntax error at a specific position
        sql = "SELECT * FROM users WHERE id = ;"  # Missing value after =
        
        with pytest.raises(SQLSyntaxError) as excinfo:
            parser.parse(sql)
            
        error = excinfo.value
        
        # Error should ideally have position or line information
        has_location = (
            hasattr(error, 'position') and error.position is not None or
            hasattr(error, 'line') and error.line is not None
        )
        
        if has_location:
            # If we have location info, it should be in the message
            if hasattr(error, 'position') and error.position is not None:
                assert f"position {error.position}" in str(error).lower()
            if hasattr(error, 'line') and error.line is not None:
                assert f"line {error.line}" in str(error).lower()
        else:
            # If not, the message should still be clear about what's wrong
            assert any(term in str(error).lower() for term in ['expected', 'missing', 'after'])

    def test_graceful_degradation(self, parser, converter):
        """Test that the system degrades gracefully for partially valid SQL."""
        # SQL with one valid statement and one invalid statement
        mixed_sql = """
        -- This part is valid
        SELECT * INTO #temp FROM users;
        
        -- This part has a syntax error
        SELECT * FROM #temp WHERE;
        """
        
        # Parsing the whole thing should fail
        with pytest.raises(SQLSyntaxError):
            parser.parse(mixed_sql)
        
        # But if we split it, we should get at least the valid statement
        statements = mixed_sql.split(';')
        valid_stmt = statements[0].strip()
        
        # This should parse successfully
        expressions = parser.parse(valid_stmt)
        
        # And we should be able to convert it
        converted = converter.convert_ast(expressions, parser)
        
        # Verify the conversion worked
        result = parser.to_sql(converted[0])
        assert "WITH temp AS" in result
        assert "FROM users" in result

    def test_unicode_handling(self, parser):
        """Test handling of Unicode characters in SQL."""
        # SQL with Unicode characters
        unicode_sql = """
        SELECT * FROM users WHERE name = '日本語';
        """
        
        # This should parse without error
        expressions = parser.parse(unicode_sql)
        
        # Verify the Unicode characters are preserved
        result = parser.to_sql(expressions[0])
        assert '日本語' in result

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\integration\test_cli.py
================================================================================
"""
Integration tests for the command-line interface.
Updated to work with the AST-based implementation.
"""
import pytest
from unittest.mock import patch
import sys
from pathlib import Path

from sql_converter.cli import main, SQLConverterApp
from sql_converter.parsers.sql_parser import SQLParser
from sql_converter.converters.cte import CTEConverter


class TestCLI:
    """Test suite for the command-line interface."""
    
    def test_cli_help(self):
        """Test the CLI help output."""
        # Patch sys.argv to simulate CLI arguments
        with patch('sys.argv', ['cli.py', '--help']):
            # Patch sys.exit to avoid exiting the test
            with patch('sys.exit') as mock_exit:
                # Patch print_help to avoid printing during tests
                with patch('argparse.ArgumentParser.print_help') as mock_print_help:
                    main()
                    mock_print_help.assert_called_once()

    def test_cli_file_conversion(self, sample_sql_file, temp_dir):
        """Test converting a file via CLI."""
        output_file = temp_dir / "output.sql"
        
        # Patch sys.argv to simulate CLI arguments
        with patch('sys.argv', [
            'cli.py',
            '-i', str(sample_sql_file),
            '-o', str(output_file),
            '-c', 'cte'
        ]):
            # Patch sys.exit to avoid exiting the test
            with patch('sys.exit'):
                main()
        
        # Verify output file was created
        assert output_file.exists()
        
        # Verify content uses CTEs
        content = output_file.read_text()
        assert "WITH temp AS" in content
        assert "FROM users" in content
        assert "SELECT name FROM temp" in content

    def test_cli_directory_conversion(self, temp_dir, sample_sql_file, complex_sql_file):
        """Test converting a directory of files via CLI."""
        input_dir = temp_dir
        output_dir = temp_dir / "output"
        
        # Patch sys.argv to simulate CLI arguments
        with patch('sys.argv', [
            'cli.py',
            '-i', str(input_dir),
            '-o', str(output_dir),
            '-c', 'cte'
        ]):
            # Patch sys.exit to avoid exiting the test
            with patch('sys.exit'):
                main()
        
        # Verify output directory was created
        assert output_dir.exists()
        
        # Verify output files were created
        assert (output_dir / "test.sql").exists()
        assert (output_dir / "complex.sql").exists()
        
        # Verify content uses CTEs
        test_content = (output_dir / "test.sql").read_text()
        assert "WITH temp AS" in test_content
        
        complex_content = (output_dir / "complex.sql").read_text()
        assert "WITH temp1 AS" in complex_content
        assert "temp2 AS" in complex_content

    def test_cli_dialect_selection(self, sample_sql_file, temp_dir):
        """Test selecting a specific SQL dialect via CLI."""
        output_file = temp_dir / "tsql_output.sql"
        
        # Patch sys.argv to simulate CLI arguments with dialect selection
        with patch('sys.argv', [
            'cli.py',
            '-i', str(sample_sql_file),
            '-o', str(output_file),
            '-c', 'cte',
            '-d', 'tsql'  # Specify T-SQL dialect
        ]):
            # Patch sys.exit to avoid exiting the test
            with patch('sys.exit'):
                main()
        
        # Verify output file was created
        assert output_file.exists()
        
        # Content should be valid - exact format might vary by dialect
        content = output_file.read_text()
        assert "WITH" in content
        assert "FROM" in content

    def test_cli_error_handling(self, temp_dir):
        """Test CLI error handling for invalid input."""
        # Create an invalid SQL file
        invalid_file = temp_dir / "invalid.sql"
        invalid_file.write_text("SELECT FROM WHERE;")  # Invalid SQL
        
        output_file = temp_dir / "output.sql"
        
        # Patch sys.argv to simulate CLI arguments
        with patch('sys.argv', [
            'cli.py',
            '-i', str(invalid_file),
            '-o', str(output_file),
            '-c', 'cte'
        ]):
            # Patch sys.exit to catch the exit code
            with patch('sys.exit') as mock_exit:
                main()
                
                # Should exit with error code
                mock_exit.assert_called()
                args = mock_exit.call_args[0]
                assert args[0] != 0  # Non-zero exit code

    def test_app_initialization(self, config_manager):
        """Test SQLConverterApp initialization."""
        # Create converter
        converter = CTEConverter()
        
        # Create app with the converter
        app = SQLConverterApp(
            converters={'cte': converter},
            config=config_manager.config
        )
        
        # Verify app has a parser instance
        assert hasattr(app, 'parser')
        assert isinstance(app.parser, SQLParser)
        
        # Verify app has the converter
        assert 'cte' in app.converters
        assert app.converters['cte'] is converter

    def test_app_summary_generation(self, converter_app, sample_sql_file, temp_dir):
        """Test generation of processing summary."""
        # Set up output paths
        output_file = temp_dir / "output.sql"
        
        # Process a file
        converter_app.process_file(sample_sql_file, output_file, ['cte'])
        
        # Get summary
        summary = converter_app.get_summary()
        
        # Verify summary statistics
        assert summary['processed_files'] == 1
        assert summary['failed_files'] == 0
        assert summary['success_rate'] == 100.0
        assert len(summary['failures']) == 0

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\integration\test_conversion.py
================================================================================
"""
Integration tests for the SQL conversion process.
Consolidated from existing integration tests and updated for AST-based implementation.
"""
import pytest
import re
from pathlib import Path

from sql_converter.cli import SQLConverterApp
from sql_converter.converters.cte import CTEConverter
from sql_converter.parsers.sql_parser import SQLParser


class TestSQLConversion:
    """Integration tests for SQL conversion."""
    
    @pytest.fixture
    def converter_app(self, config_manager):
        """Create a SQLConverterApp instance for testing."""
        # Create a parser
        parser = SQLParser(dialect=config_manager.config['parser']['dialect'])
        
        # Create a converter
        converter = CTEConverter(config=config_manager.config['cte_converter'])
        
        # Create the app
        app = SQLConverterApp(
            converters={'cte': converter},
            config=config_manager.config
        )
        return app

    def test_basic_conversion(self, converter_app, temp_dir):
        """Test basic conversion of a simple SQL file."""
        # Create input file
        input_content = """
        -- Simple SQL with a temp table
        SELECT * INTO #temp FROM users;
        SELECT name FROM #temp WHERE status = 'active';
        """
        
        input_file = temp_dir / "simple.sql"
        input_file.write_text(input_content)
        
        # Set up output file
        output_file = temp_dir / "simple_output.sql"
        
        # Process the file
        converter_app.process_file(input_file, output_file, ['cte'])
        
        # Verify the output file exists
        assert output_file.exists()
        
        # Read the output
        output_content = output_file.read_text()
        
        # Verify the conversion
        assert "WITH temp AS" in output_content
        assert "SELECT * FROM users" in output_content
        assert "SELECT name FROM temp WHERE status = 'active'" in output_content
        
        # Verify no temp tables remain
        assert "#temp" not in output_content

    def test_multiple_temp_tables(self, converter_app, temp_dir):
        """Test conversion with multiple temp tables."""
        # Create input file
        input_content = """
        -- Multiple temp tables
        SELECT * INTO #temp1 FROM users WHERE type = 'customer';
        SELECT * INTO #temp2 FROM products WHERE status = 'active';
        
        -- Query joins both temp tables
        SELECT 
            u.name,
            p.product_name,
            p.price
        FROM 
            #temp1 u
        JOIN
            #temp2 p ON u.preferred_product = p.product_id;
        """
        
        input_file = temp_dir / "multiple.sql"
        input_file.write_text(input_content)
        
        # Set up output file
        output_file = temp_dir / "multiple_output.sql"
        
        # Process the file
        converter_app.process_file(input_file, output_file, ['cte'])
        
        # Verify the output file exists
        assert output_file.exists()
        
        # Read the output
        output_content = output_file.read_text()
        
        # Verify the conversion
        assert "WITH temp1 AS" in output_content
        assert "temp2 AS" in output_content
        assert "FROM temp1 u" in output_content
        assert "JOIN temp2 p" in output_content
        
        # Verify no temp tables remain
        assert "#temp1" not in output_content
        assert "#temp2" not in output_content

    def test_fixture_based_conversion(self, converter_app, fixtures_path, temp_dir):
        """Test conversion using the fixture files."""
        # Get all input fixtures
        input_dir = fixtures_path / "input"
        fixture_files = list(input_dir.glob("*.sql"))
        
        # Skip if no fixtures found
        if not fixture_files:
            pytest.skip("No fixture files found")
            
        # Create output directory
        output_dir = temp_dir / "output"
        output_dir.mkdir(exist_ok=True)
        
        # Process each fixture
        for input_file in fixture_files:
            # Skip non-temp table fixtures
            if input_file.name == "permanent_table.sql":
                continue
                
            # Calculate output path
            output_file = output_dir / input_file.name
            
            # Process the file
            converter_app.process_file(input_file, output_file, ['cte'])
            
            # Verify output file exists
            assert output_file.exists(), f"Output file not created for {input_file.name}"
            
            # Find corresponding expected file
            expected_file = fixtures_path / "expected" / input_file.name
            
            # Skip comparison if expected file doesn't exist
            if not expected_file.exists():
                continue
                
            # Read output and expected content
            output_content = output_file.read_text()
            expected_content = expected_file.read_text()
            
            # Normalize both for comparison
            def normalize(text):
                # Remove whitespace differences
                return re.sub(r'\s+', ' ', text).strip().lower()
                
            normalized_output = normalize(output_content)
            normalized_expected = normalize(expected_content)
            
            # Verify the output matches expected
            assert normalized_output == normalized_expected, \
                f"Output doesn't match expected for {input_file.name}"

    def test_complex_nested_query(self, converter_app, temp_dir):
        """Test conversion of a complex nested query with temp tables."""
        # Create input file with complex nested queries
        input_content = """
        -- Complex nested query with CTEs and temp tables
        WITH base_data AS (
            SELECT * FROM users WHERE status = 'active'
        )
        SELECT 
            bd.id,
            bd.name,
            bd.email,
            o.order_count,
            o.total_spent
        INTO #user_stats
        FROM 
            base_data bd
        LEFT JOIN (
            SELECT 
                user_id,
                COUNT(*) as order_count,
                SUM(amount) as total_spent
            FROM orders
            WHERE order_date > '2023-01-01'
            GROUP BY user_id
        ) o ON bd.id = o.user_id;
        
        -- Query using the temp table
        SELECT * FROM #user_stats
        WHERE order_count > 5
        ORDER BY total_spent DESC;
        """
        
        input_file = temp_dir / "complex.sql"
        input_file.write_text(input_content)
        
        # Set up output file
        output_file = temp_dir / "complex_output.sql"
        
        # Process the file
        converter_app.process_file(input_file, output_file, ['cte'])
        
        # Verify the output file exists
        assert output_file.exists()
        
        # Read the output
        output_content = output_file.read_text()
        
        # Verify the original CTE is preserved
        assert "WITH base_data AS" in output_content
        
        # Verify the temp table is converted to a CTE
        assert "user_stats AS" in output_content
        
        # Verify the query references the CTE
        assert "FROM user_stats" in output_content
        
        # Verify no temp tables remain
        assert "#user_stats" not in output_content

    def test_edge_case_handling(self, converter_app, temp_dir):
        """Test handling of edge cases."""
        # Create tests for different edge cases
        edge_cases = [
            # Case 1: Multiple statements on one line
            {
                'name': 'oneline',
                'content': "SELECT * INTO #temp FROM users; SELECT * FROM #temp;",
                'check': lambda c: "WITH temp AS" in c and "SELECT * FROM temp" in c
            },
            
            # Case 2: Comments alongside code
            {
                'name': 'comments',
                'content': """
                -- Create temp table
                SELECT * INTO #temp FROM users; -- End of first statement
                /* Multi-line comment
                   spans multiple lines */
                SELECT * FROM #temp; -- Use temp table
                """,
                'check': lambda c: "WITH temp AS" in c and "SELECT * FROM temp" in c
            },
            
            # Case 3: Quoted identifiers with special characters
            {
                'name': 'quoted',
                'content': """
                SELECT * INTO #temp FROM "user.table"."column.name";
                SELECT "a.b" FROM #temp;
                """,
                'check': lambda c: "WITH temp AS" in c and 'FROM "user.table"."column.name"' in c
            }
        ]
        
        # Test each edge case
        for case in edge_cases:
            # Create input file
            input_file = temp_dir / f"{case['name']}.sql"
            input_file.write_text(case['content'])
            
            # Set up output file
            output_file = temp_dir / f"{case['name']}_output.sql"
            
            # Process the file
            converter_app.process_file(input_file, output_file, ['cte'])
            
            # Verify the output file exists
            assert output_file.exists()
            
            # Read the output and verify
            output_content = output_file.read_text()
            assert case['check'](output_content), f"Edge case '{case['name']}' failed verification"

    def test_directory_structure_preservation(self, converter_app, temp_dir):
        """Test that directory structure is preserved when processing directories."""
        # Create a nested directory structure with SQL files
        nested_dir = temp_dir / "nested"
        nested_dir.mkdir()
        
        deeper_dir = nested_dir / "deeper"
        deeper_dir.mkdir()
        
        # Create SQL files at different levels
        root_file = temp_dir / "root.sql"
        root_file.write_text("SELECT * INTO #temp FROM users; SELECT * FROM #temp;")
        
        nested_file = nested_dir / "nested.sql"
        nested_file.write_text("SELECT * INTO #temp FROM products; SELECT * FROM #temp;")
        
        deeper_file = deeper_dir / "deeper.sql"
        deeper_file.write_text("SELECT * INTO #temp FROM orders; SELECT * FROM #temp;")
        
        # Set up output directory
        output_dir = temp_dir / "output"
        
        # Process the directory
        converter_app.process_directory(temp_dir, output_dir, ['cte'])
        
        # Verify output directory structure matches input
        assert (output_dir / "root.sql").exists()
        assert (output_dir / "nested").exists()
        assert (output_dir / "nested" / "nested.sql").exists()
        assert (output_dir / "nested" / "deeper").exists()
        assert (output_dir / "nested" / "deeper" / "deeper.sql").exists()
        
        # Verify all files were converted correctly
        for output_file in [
            output_dir / "root.sql",
            output_dir / "nested" / "nested.sql",
            output_dir / "nested" / "deeper" / "deeper.sql"
        ]:
            content = output_file.read_text()
            assert "WITH temp AS" in content
            assert "#temp" not in content

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\unit\converters\test_cte_converter.py
================================================================================
"""
Unit tests for the updated AST-based CTEConverter.
Focuses on functionality rather than implementation details.
"""
import pytest
from sqlglot import exp
import re

from sql_converter.converters.cte import CTEConverter
from sql_converter.parsers.sql_parser import SQLParser
from sql_converter.exceptions import ValidationError, ConverterError, CircularDependencyError


class TestCTEConverter:
    """Test suite for the CTEConverter using AST-based implementation."""
    
    def test_basic_conversion(self, cte_converter, sql_parser, normalize_sql):
        """Test basic conversion of a temporary table to CTE."""
        sql = "SELECT * INTO #temp FROM users; SELECT * FROM #temp;"
        
        # Convert the SQL
        result = cte_converter.convert(sql)
        
        # Verify the conversion worked
        assert "WITH temp AS" in result
        assert "SELECT * FROM users" in result
        assert "SELECT * FROM temp" in result
        
        # The result should be valid SQL
        parsed_result = sql_parser.parse(result)
        assert len(parsed_result) == 1  # Should be a single WITH statement
        assert isinstance(parsed_result[0], exp.With)

    def test_ast_conversion_api(self, cte_converter, sql_parser):
        """Test the AST-based conversion API."""
        sql = "SELECT * INTO #temp FROM users; SELECT * FROM #temp;"
        
        # Parse into AST
        expressions = sql_parser.parse(sql)
        
        # Convert using the AST API
        result_expr = cte_converter.convert_ast(expressions, sql_parser)
        
        # Verify the result is a list containing a WITH expression
        assert isinstance(result_expr, list)
        assert len(result_expr) == 1
        assert isinstance(result_expr[0], exp.With)
        
        # Convert back to SQL for verification
        result_sql = sql_parser.to_sql(result_expr[0])
        
        # Verify the conversion worked
        assert "WITH temp AS" in result_sql
        assert "SELECT * FROM temp" in result_sql

    def test_multiple_temp_tables(self, cte_converter, sql_parser, load_fixture_sql):
        """Test converting multiple temp tables in a single query."""
        # Load the fixture
        sql = load_fixture_sql('input/multiple_temps.sql')
        
        # Convert using AST-based converter
        result = cte_converter.convert(sql)
        
        # Verify both temp tables are converted to CTEs
        assert "WITH temp1 AS" in result
        assert "temp2 AS" in result
        
        # Verify the references are updated
        assert "FROM temp1 t1 JOIN temp2 t2" in result
        
        # Verify this is valid SQL
        parsed_result = sql_parser.parse(result)
        assert len(parsed_result) == 1

    def test_nested_temp_tables(self, cte_converter, sql_parser, load_fixture_sql):
        """Test converting nested temp tables (one temp table references another)."""
        # Load the fixture
        sql = load_fixture_sql('input/nested_temps.sql')
        
        # Convert using AST-based converter
        result = cte_converter.convert(sql)
        
        # Verify both temp tables are converted to CTEs
        assert "WITH inner_temp AS" in result
        assert "outer_temp AS" in result
        
        # Verify the dependency order
        inner_pos = result.find("inner_temp AS")
        outer_pos = result.find("outer_temp AS")
        assert inner_pos < outer_pos  # inner_temp should be defined first
        
        # Verify references are updated
        assert "FROM inner_temp" in result

    def test_pattern_matching(self, configured_converter, sql_parser, load_fixture_sql):
        """Test custom patterns for temporary table identification."""
        # Load the fixture
        sql = load_fixture_sql('input/pattern_matching.sql')
        
        # Convert using configured converter with custom patterns
        result = configured_converter.convert(sql)
        
        # Verify conversion
        assert "WITH tmp_users AS" in result
        assert "FROM tmp_users" in result
        
        # No temp tables should remain
        assert "#tmp_users" not in result

    def test_permanent_table_preservation(self, cte_converter, sql_parser, load_fixture_sql):
        """Test that permanent tables are not converted."""
        # Load the fixture
        sql = load_fixture_sql('input/permanent_table.sql')
        
        # Convert the SQL
        result = cte_converter.convert(sql)
        
        # Verify permanent table is not converted
        assert "INTO permanent_table" in result
        assert "WITH" not in result  # No CTEs should be created

    def test_with_comments(self, cte_converter, sql_parser, load_fixture_sql):
        """Test conversion of SQL with comments."""
        # Load the fixture
        sql = load_fixture_sql('input/with_comments.sql')
        
        # Convert the SQL
        result = cte_converter.convert(sql)
        
        # Verify conversion worked
        assert "WITH commented_temp AS" in result
        assert "FROM commented_temp" in result
        
        # Comments should not interfere with the conversion
        assert "#commented_temp" not in result
        assert "#ignored_temp" not in result  # From commented-out SQL

    def test_circular_dependency_detection(self, cte_converter, sql_parser):
        """Test that circular dependencies are detected and reported."""
        # Create a circular dependency between temp tables
        sql = """
        SELECT * INTO #temp1 FROM #temp2;
        SELECT * INTO #temp2 FROM #temp1;
        SELECT * FROM #temp1;
        """
        
        # This should raise a validation error
        with pytest.raises((ValidationError, CircularDependencyError)):
            cte_converter.convert(sql)

    def test_complex_query_structure(self, cte_converter, sql_parser):
        """Test handling complex query structures."""
        sql = """
        WITH existing_cte AS (
            SELECT * FROM users WHERE status = 'active'
        )
        SELECT * INTO #temp FROM existing_cte;
        
        SELECT * FROM #temp;
        """
        
        # Convert the SQL
        result = cte_converter.convert(sql)
        
        # Verify the existing CTE is preserved
        assert "WITH existing_cte AS" in result
        
        # Verify the temp table is converted
        assert "temp AS" in result
        
        # Verify the reference is updated
        assert "FROM temp" in result

    def test_subquery_handling(self, cte_converter, sql_parser):
        """Test handling temp tables in subqueries."""
        sql = """
        SELECT * INTO #temp FROM (
            SELECT id, name 
            FROM users 
            WHERE status = 'active'
        ) active_users;
        
        SELECT * FROM #temp;
        """
        
        # Convert the SQL
        result = cte_converter.convert(sql)
        
        # Verify conversion
        assert "WITH temp AS" in result
        assert "FROM (SELECT id, name FROM users WHERE status = 'active')" in result.replace('\n', ' ')
        assert "FROM temp" in result
        
        # No temp tables should remain
        assert "#temp" not in result

    def test_dialects_support(self, cte_converter, tsql_parser):
        """Test conversion with different SQL dialects."""
        # T-SQL specific SQL
        sql = """
        SELECT [col1], [col2] INTO #temp FROM [schema].[table];
        SELECT [col1] FROM #temp WHERE [col2] > 10;
        """
        
        # Parse with TSQL parser
        expressions = tsql_parser.parse(sql)
        
        # Convert using AST-based method
        result_expr = cte_converter.convert_ast(expressions, tsql_parser)
        
        # Convert back to SQL
        result = tsql_parser.to_sql(result_expr[0])
        
        # Verify conversion worked with dialect specifics preserved
        assert "WITH temp AS" in result
        assert "[col1]" in result  # Brackets should be preserved
        assert "[schema].[table]" in result  # Schema notation should be preserved

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\unit\parsers\test_sql_parser.py
================================================================================
"""
Unit tests for the AST-based SQLParser.
Replaces the original regex-based parser tests.
"""
import pytest
from sqlglot import exp
import re

from sql_converter.parsers.sql_parser import SQLParser
from sql_converter.exceptions import SQLSyntaxError, ParserError, DialectError


class TestSQLParser:
    """Test suite for the AST-based SQLParser."""
    
    def test_parser_initialization(self):
        """Test that the parser initializes with different dialects."""
        # Default initialization
        parser = SQLParser()
        assert parser.dialect_name == 'ansi'
        assert parser.dialect == 'ansi'
        
        # Explicit dialect initialization
        parser = SQLParser(dialect='tsql')
        assert parser.dialect_name == 'tsql'
        assert parser.dialect == 'tsql'
        
        # Unknown dialect falls back to ANSI
        parser = SQLParser(dialect='unknown')
        assert parser.dialect_name == 'unknown'
        assert parser.dialect == 'ansi'  # Falls back to ANSI

    def test_basic_parsing(self, sql_parser):
        """Test basic SQL parsing capabilities."""
        sql = "SELECT * FROM users WHERE id = 1;"
        
        # Parse the SQL into AST expressions
        expressions = sql_parser.parse(sql)
        
        # Verify the result is a list of expressions
        assert isinstance(expressions, list)
        assert len(expressions) == 1
        
        # Verify the expression is a Select
        assert isinstance(expressions[0], exp.Select)
        
        # Verify the structure
        select_expr = expressions[0]
        assert select_expr.find(exp.Star)  # Has a * selection
        
        # Check the table reference
        table = select_expr.find(exp.Table)
        assert table.name == 'users'
        
        # Check the WHERE clause
        where = select_expr.find(exp.Where)
        assert where is not None

    def test_multi_statement_parsing(self, sql_parser):
        """Test parsing multiple SQL statements."""
        sql = """
        SELECT * FROM users;
        UPDATE users SET status = 'active' WHERE id = 1;
        """
        
        # Parse multiple statements
        expressions = sql_parser.parse(sql)
        
        # Verify we got two expressions
        assert len(expressions) == 2
        assert isinstance(expressions[0], exp.Select)
        assert isinstance(expressions[1], exp.Update)

    def test_statement_splitting(self, sql_parser):
        """Test splitting SQL into individual statements."""
        sql = """
        SELECT * FROM table; 
        -- Comment
        INSERT INTO #temp VALUES (1);
        """
        
        # Split statements using the parser
        statements = sql_parser.split_statements(sql)
        
        # Verify the results
        assert len(statements) == 2
        assert "SELECT" in statements[0]
        assert "INSERT" in statements[1]
        
        # Verify comments are handled correctly
        assert "Comment" not in statements[0]
        assert "Comment" not in statements[1]

    def test_dialect_specific_parsing(self, tsql_parser, mysql_parser):
        """Test dialect-specific SQL features."""
        # Test T-SQL specific features
        tsql = "SELECT [col.name] FROM [dbo].[table];"
        
        tsql_expressions = tsql_parser.parse(tsql)
        assert len(tsql_expressions) == 1
        
        # Test MySQL specific features
        mysql = "SELECT * FROM `users` LIMIT 10;"
        
        mysql_expressions = mysql_parser.parse(mysql)
        assert len(mysql_expressions) == 1
        
        # Limit should be preserved
        mysql_sql = mysql_parser.to_sql(mysql_expressions[0])
        assert "LIMIT" in mysql_sql.upper()

    def test_validate_sql(self, sql_parser):
        """Test SQL validation capabilities."""
        # Test valid SQL
        valid_sql = "SELECT * FROM users WHERE id = 1;"
        sql_parser.validate_sql(valid_sql)  # Should not raise
        
        # Test invalid SQL
        invalid_sql = "SELECT FROM WHERE;"
        with pytest.raises(SQLSyntaxError):
            sql_parser.validate_sql(invalid_sql)
        
        # Test empty SQL
        with pytest.raises(SQLSyntaxError) as excinfo:
            sql_parser.validate_sql("")
        assert "empty sql statement" in str(excinfo.value).lower()

    def test_syntax_error_detection(self, sql_parser):
        """Test that syntax errors are properly detected and reported."""
        invalid_sql = "SELECT FROM users;"
        
        # This should raise a SQLSyntaxError
        with pytest.raises(SQLSyntaxError) as excinfo:
            sql_parser.parse(invalid_sql)
        
        # Verify error contains position information
        error_msg = str(excinfo.value).lower()
        assert "syntax error" in error_msg

    def test_find_table_references(self, sql_parser):
        """Test finding table references in SQL."""
        sql = """
        SELECT u.id, o.order_id 
        FROM users u
        JOIN orders o ON u.id = o.user_id
        LEFT JOIN profiles p ON u.id = p.user_id;
        """
        
        # Find table references
        table_refs = sql_parser.find_table_references(sql)
        
        # Verify we found all tables
        table_names = [ref['table'] for ref in table_refs]
        assert 'users' in table_names
        assert 'orders' in table_names
        assert 'profiles' in table_names
        
        # Verify aliases
        aliases = [ref['alias'] for ref in table_refs]
        assert 'u' in aliases
        assert 'o' in aliases
        assert 'p' in aliases
        
        # Verify contexts
        contexts = [ref['context'] for ref in table_refs]
        assert 'FROM' in contexts
        assert any('JOIN' in ctx for ctx in contexts)

    def test_find_temp_tables(self, sql_parser):
        """Test finding temporary tables in SQL."""
        sql = """
        SELECT * INTO #temp1 FROM users;
        CREATE TEMP TABLE #temp2 AS SELECT * FROM orders;
        SELECT t1.id, t2.order_id FROM #temp1 t1 JOIN #temp2 t2 ON t1.id = t2.user_id;
        """
        
        # Find temp tables with patterns
        temp_tables = sql_parser.find_temp_tables(sql, ['#.*'])
        
        # Verify we found both temp tables
        temp_names = [info['name'] for info in temp_tables]
        assert '#temp1' in temp_names
        assert '#temp2' in temp_names
        
        # Verify types are correct
        types = {info['name']: info['type'] for info in temp_tables}
        assert types['#temp1'] == 'SELECT_INTO'
        
        # At least one of the temp tables should be recognized as CREATE_TEMP type
        assert any(t['type'] in ('CREATE_TEMP', 'CREATE_TEMP_AS') for t in temp_tables)

    def test_replace_references(self, sql_parser):
        """Test replacing table references in an AST."""
        sql = "SELECT * FROM #temp WHERE id = 1;"
        
        # Parse SQL
        expressions = sql_parser.parse(sql)
        
        # Replace references
        replacements = {'#temp': 'cte_temp'}
        modified = sql_parser.replace_references(expressions[0], replacements)
        
        # Convert back to SQL and verify the replacement
        modified_sql = sql_parser.to_sql(modified)
        assert '#temp' not in modified_sql
        assert 'cte_temp' in modified_sql
        
        # Original expression should be unchanged
        original_sql = sql_parser.to_sql(expressions[0])
        assert '#temp' in original_sql

    def test_generate_cte(self, sql_parser):
        """Test generating a CTE from a subquery."""
        sql = "SELECT * FROM users WHERE id = 1;"
        
        # Parse SQL
        expressions = sql_parser.parse(sql)
        select_expr = expressions[0]
        
        # Generate a CTE
        cte = sql_parser.generate_cte('user_cte', select_expr)
        
        # Verify result is a With expression
        assert isinstance(cte, exp.With)
        
        # Verify CTE name and definition is in the SQL
        cte_sql = sql_parser.to_sql(cte)
        assert 'WITH user_cte AS' in cte_sql.upper()
        assert 'SELECT * FROM users WHERE id = 1' in cte_sql

    def test_comment_handling(self, sql_parser):
        """Test that comments are properly handled."""
        sql = """
        -- This is a comment
        SELECT * FROM users; /* This is another comment */
        -- This is a final comment
        """
        
        # Parse SQL
        expressions = sql_parser.parse(sql)
        
        # Verify the comment doesn't interfere with the parse
        assert len(expressions) == 1
        assert isinstance(expressions[0], exp.Select)
        
        # Convert back to SQL
        result_sql = sql_parser.to_sql(expressions[0])
        
        # Comments should be removed in the parsed output
        assert "This is a comment" not in result_sql
        assert "This is another comment" not in result_sql

    def test_to_sql_format_consistency(self, sql_parser):
        """Test that to_sql produces consistent SQL format."""
        sql = "SELECT id, name FROM users WHERE status = 'active' ORDER BY name;"
        
        # Parse and convert back to SQL
        expressions = sql_parser.parse(sql)
        result_sql = sql_parser.to_sql(expressions[0])
        
        # Normalize for comparison (different formatters may have whitespace differences)
        def normalize(s):
            return re.sub(r'\s+', ' ', s).strip().lower()
        
        # Core elements should be preserved
        normalized_original = normalize(sql)
        normalized_result = normalize(result_sql)
        
        assert "select" in normalized_result
        assert "from users" in normalized_result
        assert "where status = 'active'" in normalized_result
        assert "order by" in normalized_result

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\tests\unit\utils\test_config.py
================================================================================
import pytest
from unittest.mock import mock_open, patch
from sql_converter.utils.config import ConfigManager

def test_config_loading():
    yaml_content = """
    converters:
      - cte
      - pivot
    logging:
      level: DEBUG
    """
    # Ensure the mock correctly simulates a config file
    with patch("builtins.open", mock_open(read_data=yaml_content)):
        # Patch Path.exists and Path.is_file to return True for any path
        with patch("pathlib.Path.exists", return_value=True):
            with patch("pathlib.Path.is_file", return_value=True):
                manager = ConfigManager()
                # Force a specific path that will be mocked
                manager.config_paths = [manager.config_paths[0]]  # Just use the first path
                manager.load_config()
                
                # Verify the config was loaded correctly
                assert 'pivot' in manager.get('converters')
                assert manager.get('logging.level') == 'DEBUG'

def test_config_priority():
    manager = ConfigManager()
    manager.config = {'converters': ['base']}
    manager.update_from_cli({'convert': ['cte']})
    assert manager.config['converters'] == ['cte']

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\utils\config.py
================================================================================
"""
Enhanced configuration management for SQL Converter with AST-based parsing support.

This module provides a comprehensive configuration system for the SQL Converter
application, with support for AST-based parsing and transformation options.
"""
import os
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Set
import yaml
from dotenv import load_dotenv

from sql_converter.exceptions import ConfigError


class ConfigManager:
    """
    Manages configuration from multiple sources with precedence rules,
    including enhanced support for AST-based parsing options.
    """
    
    # Default configuration
    DEFAULT_CONFIG = {
        'converters': ['cte'],
        'logging': {
            'level': 'INFO',
            'file': 'conversions.log',
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            'console': True
        },
        'parser': {
            'dialect': 'ansi',
            'optimization_level': 1,
            'schema_aware': False,
            'error_handling': 'strict',
            'pretty_print': {
                'enabled': True,
                'indent_spaces': 2,
                'uppercase_keywords': True,
                'max_line_length': 100
            }
        },
        'cte_converter': {
            'indent_spaces': 2,
            'temp_table_patterns': ['#?temp_*', '#?tmp_*', '#.*'],
            'cte_naming': {
                'strip_prefix': True,
                'style': 'original'
            },
            'dependency_handling': {
                'detect_cycles': True,
                'auto_break_cycles': False
            },
            'ast': {
                'preserve_comments': True,
                'preserve_formatting': False
            }
        },
        'output': {
            'default_output_dir': './converted_sql',
            'overwrite': True,
            'backup': True,
            'format': True,
            'formatting': {
                'indent_spaces': 2,
                'uppercase_keywords': True,
                'max_line_length': 80,
                'comma_style': 'end',
                'align_columns': True
            }
        },
        'advanced': {
            'parallelism': 0,
            'max_memory_mb': 0,
            'timeout_seconds': 0
        }
    }
    
    # Valid configuration values
    VALID_DIALECTS = {'ansi', 'tsql', 'mysql', 'postgresql', 'oracle', 'snowflake', 'redshift'}
    VALID_OPTIMIZATION_LEVELS = {0, 1, 2}
    VALID_ERROR_HANDLING = {'strict', 'relaxed', 'recovery'}
    VALID_LOG_LEVELS = {'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'}
    VALID_NAMING_STYLES = {'original', 'snake_case', 'camelCase'}
    VALID_COMMA_STYLES = {'end', 'start'}
    
    def __init__(self):
        """Initialize the configuration manager with enhanced AST options."""
        self.config: Dict[str, Any] = {}
        self.logger = logging.getLogger(__name__)
        
        # Try to load environment variables
        try:
            load_dotenv()  # Load environment variables
        except Exception as e:
            self.logger.warning(f"Failed to load environment variables: {str(e)}")
        
        # Default search paths for config files
        self.config_paths = [
            Path("sql_converter/config/default.yml"),
            Path(os.getenv("SQL_CONVERTER_CONFIG", "")),
            Path("~/.config/sql_converter/config.yml").expanduser(),
            Path("./sql_converter.yml"),
        ]
        
        # Initialize with default configuration
        self.config = self.DEFAULT_CONFIG.copy()

    def load_config(self) -> None:
        """
        Load configuration from first found valid config file,
        with support for AST-based parsing options.
        
        Raises:
            ConfigError: When config loading fails critically
        """
        loaded = False
        errors = []
        
        # Try each path in order
        for path in self.config_paths:
            if not path or not path.exists() or not path.is_file():
                continue
                
            try:
                with open(path, 'r') as f:
                    loaded_config = yaml.safe_load(f)
                    
                # Validate config structure
                if not isinstance(loaded_config, dict):
                    self.logger.warning(f"Invalid config format in {path}: not a dictionary")
                    errors.append(f"Config at {path} is not a dictionary")
                    continue
                
                # Merge with defaults, with file config taking precedence
                self._recursive_merge(self.config, loaded_config)
                
                self.logger.info(f"Loaded config from {path}")
                loaded = True
                break
                
            except Exception as e:
                error_msg = f"Failed to load config from {path}: {str(e)}"
                self.logger.warning(error_msg)
                errors.append(error_msg)
        
        # If no config loaded, use defaults but raise warning
        if not loaded:
            self.logger.warning("No valid config file found, using defaults")
            
            # If there were critical errors in configuration loading, raise exception
            if any("Permission denied" in err for err in errors):
                raise ConfigError(
                    "Cannot access configuration files due to permission issues",
                    "\n".join(errors)
                )
        
        # Process environment variable overrides for key settings
        self._apply_env_overrides()

    def _apply_env_overrides(self) -> None:
        """Apply configuration overrides from environment variables."""
        # Map of environment variables to config paths
        env_mappings = {
            'SQL_CONVERTER_DIALECT': 'parser.dialect',
            'SQL_CONVERTER_LOG_LEVEL': 'logging.level',
            'SQL_CONVERTER_LOG_FILE': 'logging.file',
            'SQL_CONVERTER_CONVERTERS': 'converters',
            'SQL_CONVERTER_OUTPUT_DIR': 'output.default_output_dir',
            'SQL_CONVERTER_OPTIMIZATION': 'parser.optimization_level',
        }
        
        # Apply each override if environment variable exists
        for env_var, config_path in env_mappings.items():
            env_value = os.getenv(env_var)
            if env_value is not None:
                # Special handling for lists
                if config_path == 'converters':
                    converters = [c.strip() for c in env_value.split(',')]
                    self.set(config_path, converters)
                # Special handling for numeric values
                elif config_path == 'parser.optimization_level':
                    try:
                        opt_level = int(env_value)
                        if opt_level in self.VALID_OPTIMIZATION_LEVELS:
                            self.set(config_path, opt_level)
                        else:
                            self.logger.warning(
                                f"Invalid optimization level in {env_var}: {env_value}. "
                                f"Must be one of {self.VALID_OPTIMIZATION_LEVELS}"
                            )
                    except ValueError:
                        self.logger.warning(
                            f"Invalid numeric value in {env_var}: {env_value}"
                        )
                else:
                    self.set(config_path, env_value)

    def get(self, key: str, default: Optional[Any] = None) -> Any:
        """
        Get config value using dot notation (e.g. 'parser.dialect').
        
        Args:
            key: Config key using dot notation
            default: Default value if key not found
            
        Returns:
            Config value or default
            
        Raises:
            ConfigError: When key is invalid
        """
        if not key:
            raise ConfigError("Empty configuration key provided")
            
        keys = key.split('.')
        value = self.config
        
        try:
            for k in keys:
                if not isinstance(value, dict):
                    self.logger.debug(f"Config path '{key}' traversal failed at '{k}': not a dictionary")
                    return default
                value = value.get(k)
                if value is None:
                    return default
            return value
        except Exception as e:
            self.logger.debug(f"Error retrieving config value for '{key}': {str(e)}")
            return default

    def set(self, key: str, value: Any) -> None:
        """
        Set a configuration value using dot notation.
        
        Args:
            key: Config key using dot notation
            value: Value to set
            
        Raises:
            ConfigError: When key is invalid
        """
        if not key:
            raise ConfigError("Empty configuration key provided")
            
        keys = key.split('.')
        config = self.config
        
        # Traverse to the second-to-last key
        for k in keys[:-1]:
            if k not in config:
                config[k] = {}
            elif not isinstance(config[k], dict):
                config[k] = {}
            config = config[k]
            
        # Set the value
        config[keys[-1]] = value

    def update_from_cli(self, cli_args: Dict[str, Any]) -> None:
        """
        Merge CLI arguments into config, with support for AST-specific options.
        
        Args:
            cli_args: CLI arguments dictionary
            
        Raises:
            ConfigError: When CLI arguments are invalid
        """
        if not isinstance(cli_args, dict):
            raise ConfigError(f"CLI arguments must be a dictionary, got {type(cli_args).__name__}")
            
        try:
            # Apply CLI arguments with proper validation
            
            # Converters
            if 'convert' in cli_args:
                converters = cli_args['convert']
                if not isinstance(converters, list):
                    raise ConfigError(f"'convert' must be a list, got {type(converters).__name__}")
                self.config['converters'] = converters
            
            # Input/output paths
            if 'input' in cli_args:
                input_path = cli_args['input']
                if not isinstance(input_path, (str, Path)):
                    raise ConfigError(f"'input' must be a string or Path, got {type(input_path).__name__}")
                self.config['input_path'] = input_path
                
            if 'output' in cli_args:
                output_path = cli_args['output']
                if not isinstance(output_path, (str, Path)):
                    raise ConfigError(f"'output' must be a string or Path, got {type(output_path).__name__}")
                self.config['output_path'] = output_path
            
            # AST-specific options
            if 'dialect' in cli_args:
                dialect = cli_args['dialect']
                if dialect not in self.VALID_DIALECTS:
                    raise ConfigError(
                        f"Invalid SQL dialect: {dialect}. "
                        f"Must be one of {', '.join(self.VALID_DIALECTS)}"
                    )
                self.set('parser.dialect', dialect)
                
            if 'optimize' in cli_args:
                optimize = cli_args['optimize']
                if not isinstance(optimize, int) or optimize not in self.VALID_OPTIMIZATION_LEVELS:
                    raise ConfigError(
                        f"Invalid optimization level: {optimize}. "
                        f"Must be one of {self.VALID_OPTIMIZATION_LEVELS}"
                    )
                self.set('parser.optimization_level', optimize)
                
            # Logging options
            if 'verbose' in cli_args and cli_args['verbose']:
                self.set('logging.level', 'DEBUG')
                
        except Exception as e:
            if isinstance(e, ConfigError):
                raise
            raise ConfigError(f"Error updating configuration from CLI: {str(e)}")

    def validate_config(self) -> List[str]:
        """
        Validate the loaded configuration, including AST-based options.
        
        Returns:
            List of validation errors (empty if valid)
            
        Raises:
            ConfigError: When validation fails critically
        """
        errors = []
        
        # Check for required sections
        if 'converters' not in self.config:
            errors.append("Missing 'converters' section in configuration")
            
        # Validate converters
        converters = self.config.get('converters', [])
        if not isinstance(converters, list):
            errors.append(f"'converters' must be a list, got {type(converters).__name__}")
        elif not converters:
            errors.append("No converters specified in configuration")
            
        # Validate converter-specific configs
        for converter in converters:
            converter_config_key = f"{converter}_converter"
            converter_config = self.config.get(converter_config_key)
            
            if converter_config is not None and not isinstance(converter_config, dict):
                errors.append(f"'{converter_config_key}' must be a dictionary")
        
        # Validate parser config
        parser_config = self.config.get('parser', {})
        if not isinstance(parser_config, dict):
            errors.append(f"'parser' must be a dictionary, got {type(parser_config).__name__}")
        else:
            # Check dialect
            dialect = parser_config.get('dialect')
            if dialect and dialect not in self.VALID_DIALECTS:
                errors.append(
                    f"Invalid SQL dialect: {dialect}. "
                    f"Must be one of {', '.join(self.VALID_DIALECTS)}"
                )
                
            # Check optimization level
            opt_level = parser_config.get('optimization_level')
            if opt_level is not None:
                if not isinstance(opt_level, int) or opt_level not in self.VALID_OPTIMIZATION_LEVELS:
                    errors.append(
                        f"Invalid optimization level: {opt_level}. "
                        f"Must be one of {self.VALID_OPTIMIZATION_LEVELS}"
                    )
                    
            # Check error handling
            error_handling = parser_config.get('error_handling')
            if error_handling and error_handling not in self.VALID_ERROR_HANDLING:
                errors.append(
                    f"Invalid error handling mode: {error_handling}. "
                    f"Must be one of {', '.join(self.VALID_ERROR_HANDLING)}"
                )
                
        # Validate logging config
        logging_config = self.config.get('logging', {})
        if not isinstance(logging_config, dict):
            errors.append(f"'logging' must be a dictionary, got {type(logging_config).__name__}")
        else:
            # Check log level
            log_level = logging_config.get('level')
            if log_level and log_level not in self.VALID_LOG_LEVELS:
                errors.append(
                    f"Invalid log level: '{log_level}'. "
                    f"Must be one of {', '.join(self.VALID_LOG_LEVELS)}"
                )
                
            # Check log file
            log_file = logging_config.get('file')
            if log_file and not isinstance(log_file, (str, Path)):
                errors.append(
                    f"'logging.file' must be a string or Path, got {type(log_file).__name__}"
                )
                
        # Validate CTE converter config
        cte_config = self.config.get('cte_converter', {})
        if not isinstance(cte_config, dict):
            errors.append(f"'cte_converter' must be a dictionary, got {type(cte_config).__name__}")
        else:
            # Check temp table patterns
            patterns = cte_config.get('temp_table_patterns')
            if patterns is not None:
                if not isinstance(patterns, list):
                    errors.append(
                        f"'cte_converter.temp_table_patterns' must be a list, "
                        f"got {type(patterns).__name__}"
                    )
                elif not patterns:
                    errors.append("No temp table patterns specified")
                    
            # Check naming style
            naming = cte_config.get('cte_naming', {})
            if naming and isinstance(naming, dict):
                style = naming.get('style')
                if style and style not in self.VALID_NAMING_STYLES:
                    errors.append(
                        f"Invalid naming style: {style}. "
                        f"Must be one of {', '.join(self.VALID_NAMING_STYLES)}"
                    )
                
        # Validate output config
        output_config = self.config.get('output', {})
        if not isinstance(output_config, dict):
            errors.append(f"'output' must be a dictionary, got {type(output_config).__name__}")
        else:
            # Check output directory
            output_dir = output_config.get('default_output_dir')
            if output_dir and not isinstance(output_dir, (str, Path)):
                errors.append(
                    f"'output.default_output_dir' must be a string or Path, "
                    f"got {type(output_dir).__name__}"
                )
                
            # Check formatting options
            formatting = output_config.get('formatting', {})
            if formatting and isinstance(formatting, dict):
                comma_style = formatting.get('comma_style')
                if comma_style and comma_style not in self.VALID_COMMA_STYLES:
                    errors.append(
                        f"Invalid comma style: {comma_style}. "
                        f"Must be one of {', '.join(self.VALID_COMMA_STYLES)}"
                    )
                
        # Return all validation errors
        return errors
            
    def _recursive_merge(self, base: Dict[str, Any], overlay: Dict[str, Any]) -> None:
        """
        Recursively merge overlay dictionary into base dictionary.
        
        Args:
            base: Base dictionary to merge into
            overlay: Overlay dictionary with values to merge
        """
        for key, value in overlay.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                # Recursively merge nested dictionaries
                self._recursive_merge(base[key], value)
            else:
                # Otherwise replace or add the value
                base[key] = value
                
    def get_for_converter(self, converter_name: str) -> Dict[str, Any]:
        """
        Get configuration specific to a converter, including AST options.
        
        Args:
            converter_name: Name of the converter
            
        Returns:
            Configuration dictionary for the converter
        """
        config_key = f"{converter_name}_converter"
        converter_config = self.config.get(config_key, {})
        
        # Add parser-related AST options that converters might need
        parser_config = self.config.get('parser', {})
        ast_config = {
            'dialect': parser_config.get('dialect', 'ansi'),
            'optimization_level': parser_config.get('optimization_level', 1),
            'pretty_print': parser_config.get('pretty_print', {}),
        }
        
        # Merge configurations
        result = {**converter_config, 'ast': {**ast_config, **converter_config.get('ast', {})}}
        return result

    def get_all_sections(self) -> Dict[str, Any]:
        """
        Get all configuration sections (for debugging).
        
        Returns:
            Dictionary with all configuration sections
        """
        return self.config.copy()

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\utils\logging.py
================================================================================
"""
Enhanced logging configuration for SQL Converter with AST-based parsing support.

This module provides a more comprehensive logging system supporting
additional diagnostic information about AST operations.
"""
import logging
import sys
from typing import Optional, Dict, Any, List


class SQLNodeAdapter(logging.LoggerAdapter):
    """
    Adapter to include AST node information in logging messages.
    This enriches log messages with AST context when available.
    """
    
    def process(self, msg, kwargs):
        """Process the log message to include AST node information."""
        extra = kwargs.get('extra', {})
        node_info = ""
        
        # Extract node information if available
        if 'node_type' in extra:
            node_info = f" [Node: {extra['node_type']}"
            if 'node_name' in extra:
                node_info += f", {extra['node_name']}"
            node_info += "]"
            
        # Extract operation information if available
        operation_info = ""
        if 'operation' in extra:
            operation_info = f" [Operation: {extra['operation']}]"
            
        # Combine all information
        enhanced_msg = f"{msg}{node_info}{operation_info}"
        
        return enhanced_msg, kwargs


def setup_logging(
    level: str = 'INFO', 
    log_file: Optional[str] = None,
    log_format: Optional[str] = None,
    console: bool = True
) -> None:
    """
    Configure the logging system with enhanced AST-related options.
    
    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Path to log file (None for console only)
        log_format: Custom log format string
        console: Whether to output logs to console
    """
    # Convert level string to logging constant
    try:
        numeric_level = getattr(logging, level.upper(), logging.INFO)
    except AttributeError:
        print(f"Invalid logging level: {level}, defaulting to INFO")
        numeric_level = logging.INFO
    
    # Set default format if not provided
    if not log_format:
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(numeric_level)
    
    # Remove any existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Create and add handlers
    handlers = []
    
    # Add console handler if requested
    if console:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(numeric_level)
        console_handler.setFormatter(logging.Formatter(log_format))
        handlers.append(console_handler)
    
    # Add file handler if requested
    if log_file:
        try:
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(numeric_level)
            file_handler.setFormatter(logging.Formatter(log_format))
            handlers.append(file_handler)
        except Exception as e:
            print(f"Error setting up log file: {e}")
            # Fall back to console logging
            if not console:
                console_handler = logging.StreamHandler(sys.stdout)
                console_handler.setLevel(numeric_level)
                console_handler.setFormatter(logging.Formatter(log_format))
                handlers.append(console_handler)
    
    # Add all handlers to root logger
    for handler in handlers:
        root_logger.addHandler(handler)
    
    # Log the setup
    logging.info(f"Logging initialized. Level: {level}, File: {log_file if log_file else 'None'}")


def get_ast_logger(name: str) -> logging.LoggerAdapter:
    """
    Get a logger adapter for AST operations.
    
    Args:
        name: Logger name (typically __name__)
        
    Returns:
        LoggerAdapter with AST node context support
    """
    logger = logging.getLogger(name)
    return SQLNodeAdapter(logger, {})


def log_ast_operation(
    logger: logging.LoggerAdapter,
    message: str,
    operation: str,
    node_type: Optional[str] = None,
    node_name: Optional[str] = None,
    level: str = 'DEBUG',
    **extra
):
    """
    Log an AST operation with relevant context.
    
    Args:
        logger: Logger adapter to use
        message: Log message
        operation: Operation being performed
        node_type: Type of AST node involved
        node_name: Name or identifier of the node
        level: Logging level
        **extra: Additional context information
    """
    # Combine context
    context = {
        'operation': operation,
        **extra
    }
    
    if node_type:
        context['node_type'] = node_type
    if node_name:
        context['node_name'] = node_name
    
    # Log with appropriate level
    level_method = getattr(logger, level.lower(), logger.debug)
    level_method(message, extra=context)


def log_ast_transformation(
    logger: logging.LoggerAdapter,
    source_type: str,
    target_type: str,
    transformation: str,
    success: bool,
    details: Optional[Dict[str, Any]] = None
):
    """
    Log an AST transformation operation.
    
    Args:
        logger: Logger adapter to use
        source_type: Source node type
        target_type: Target node type
        transformation: Transformation being performed
        success: Whether the transformation succeeded
        details: Additional details about the transformation
    """
    status = "succeeded" if success else "failed"
    message = f"Transformation {transformation} {status}: {source_type} → {target_type}"
    
    level = "INFO" if success else "WARNING"
    
    # Log with transformation details
    log_ast_operation(
        logger,
        message,
        operation=transformation,
        node_type=source_type,
        level=level,
        target_type=target_type,
        details=details or {}
    )


def format_node_path(path: List[Any]) -> str:
    """
    Format a node path for logging.
    
    Args:
        path: List of nodes in the path
        
    Returns:
        Formatted string representation of the path
    """
    if not path:
        return "[]"
    
    formatted = []
    for node in path:
        try:
            node_type = type(node).__name__
            formatted.append(node_type)
        except:
            formatted.append("Unknown")
    
    return " → ".join(formatted)

################################################################################

File: C:\Users\User\python_code\sql_conversion_test\sql-query-converter\sql_converter\utils\__init__.py
================================================================================


################################################################################

